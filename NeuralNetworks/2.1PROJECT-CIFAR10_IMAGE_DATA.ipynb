{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I evaluated the CIFAR10 dataset and trained an necessarily large Deep Neural Network using several bad practices and some good ones. The purpose behind this exercise was to demonstrate which combination of acitvation functions and optimizers help produce a faster, more accurate model. \n",
    "\n",
    "The aim of the notebook was: \n",
    "1. Use Tensorboard to help find the learning rate whenever you change the network architecture\n",
    "2. I used ELU and SELU as the activation functions.\n",
    "3. Explore what Batch Normalization does and its affect on speeding up trying by allowing a larger learning rate.\n",
    "4. Apply AlphaDropout regularization with SELU as the activation function model and then use Monte Carlo (MC) Dropout. \n",
    "5. Use 1Cycle Scheduling \n",
    "6. Understand the importance of convergence speed and finding the best model with few epochs.  \n",
    "\n",
    "\n",
    "<i>Please be mindful, in order for me to reduce run time of training the deep neural networks, I have reduced the batch_size and have only kept the best learning rates within the notebook, based on the 5 learning rates I applied per neural network architecture change. The purpose of this notebook is to understand what can be done to improve wall time and convergence rather than sitting around waiting for your model to train. I have kept one example below on how I searched for the best learning rate, by simply using a for loop and observing the best model (i.e. High Accuracy and Low Loss) for only 10 epochs. Furthermore, the accuracy of the models presented here can be signifcantly improved by reducing the number of layers and using a Conventional Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#useful imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import os\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_val, y_train_val), (X_test, y_test) = keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "(10000, 32, 32, 3)\n",
      "(32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_val.shape)\n",
    "print(X_test.shape)\n",
    "print(X_train_val[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAF8AAABfCAYAAACOTBv1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO292XIkWZKm953VzHwDEEtG1j7T08NbiozM2/IdKBTeUYTPMcPmNJvd1VWVlRkLAIe723IW5YUecyCqRyK7I2WkLphW5ekIwOEwVztH9ddff1UzIsLPx1/nsH/tE/j/8/Gz8f+Kx8/G/ysePxv/r3j8bPy/4uG/9MP/5X//P0Wk8vTpB54+fEeaR04fv2cZz4gYRCzWBfa339Bt9mx2B+5evyPESL+/pRt2OGfxwQLCpw9/4P7DH5mnC5/e/5FpPFNTos4LCNhqMVh8DPRDj7UW6yPGOW7vXvG7v/kPDJstr16/4fbuFVIKaTpTc+bDhx94/8N31FoRyYhUqhRKyaSU+P7P33E8PpKWWf9uLeSSqbVSK5RikQrjmJjGTKmVeUmUUhEpVClYa9ntNnRdJEbPsIkYA8KMkKkVagERIWcoWe34v/2v/4f5NxtfROAFEjXm+QEGEQMY/b/5774/+gbtISDrP9f3bG/8/GywxmCMwViLteuzxRir32+vFwPtNzEGrNWLXKtVo1T57HeuL/z8r392XN+vfW2MQTCfnfOXD7k+/RiM/6Lxl3kCEYwIfdeRDeRhg0MQPGI8zgW2+1v67Z4YezBClQqSMZJBvwSp1GWkzCNlnpCUIBeCdQy9xxrL0G0JPtINPbvDDuc9/bAhdB3b3Y5v3n1L1/VsdzuGTY/UQvGGWgveGbabQVd8zYgItWZyzszzRFoypQiXy5nxMuo5GouxYI0BLNVAiAbBUWrF+EQtlVwSKQvWWry3OGcwFkQvi15gHELV3VSEkqGUn2D8eRwBsAJD11OspQ4bkjWICWA7rI/sDrf02wPG6i4QKc34CaqAVDXUMpKnkTxP1JQgZ0Ls2PYDwQfubt/Q9xs2+y23r+8IMbA/3LDZbIhdx2F/wHmPdx7rvBq6j4hUDvsdtb69Gl1Eri5nHEeOxxPjuFArCPeUajDWYa1tO1gvBDisg1orPjhqrcyLgblgrcF5i/O6I6Ei0HaqoxSh5EIplZKFUr5o+y8bf/URIvLSeSAC1llsCFgXMFbjtlShSNHX14oue9FnBGctIQQMld12R46RTdezGzaEENjf3DD0W/rtwLDd4oOnH3q6vieEgHMOZy3WGGx715f+YHUt1jrdse1/ORdi7Om6nhAixjhd9Yi6leYORcBYi2vupkpp7sy8+Bs0nyRXt1KrYExt8UP0eXWxX238dumkFEou5FxIS2FZMkPcst0dsC4AhnlZEKmUqiuk6yNdH7B2XRnCdrvB8RaDEPyvsNawHTbsN3u8D2y3B2LoscFiO6/v03XEEHTLO6fvpZ8YaiEvC7VWStW/bYzBO4exFu89MfYYE7i9ecM0Zmo1hPADpYAxFYwujFJ0t3gf8UF3k8tQSqGUmck0W5DReGd1kQmUkiklk3NhmVML+j/R+Lp61w8qSJV2ZTWQxdhhrCeLnuSKIKw17cMUjYotiIUQsJsN3ll2254QPNthx2F3wDnPMOzwPlKtUK2AgeA93lksYNYPI/ofqZVaixqoXQBjDM46TNud3kdCEPp+YLPZ0XVHnPO6O2jBlNLcle4E711Dcw5j5Opi9HOs+79eP1cpujBzczm1Pv/s642/Oq3V+CJYY/XEq5DmGesKxncEa7CxI8Qtzjtu7g7sDwesNXir6KPmgZoWrDXE4HDOkq3jcVqwJvM0V4zxanxTqAhpnigp0fnAfhgIzrHfbNgOA1Kf0VgphZQSAszTAoBzHu8CKWdyFqwNeN/RdRtqhXkZKTmr0a3DGl2tCkELc5qpJVFqaoEZvLeE4K6rQF2saCxpCEdkRYVfvgBfNH7NqX1RMLVCVX/qXUCqMF9GXbG7QIiOYTNw8/qWEAO7mwOb3RZjDc75z1zFisFFKtO4cDmP1CKkrDi5mkoxhVISH3/4nuPjA4fNll9/8w2bvuc3v/gFXeiQ9cO34DrNE6VUpilRcsFaj7OeKsKyVJzriGFgszmAcaScWNIFYwzWecBQRZjTQq2Zab5QSiKXhLXgnCVGT9dFam0XWypVBKlQy7NncM426PuVxl/djmnOa8XS1jqcc3jvcN4Tu0jXdfqIER881loqghGjRkefpVREGiSTwmWeOV0mam3Gr1CpVFPIOfFwfOLh0wM1Fe62O4xASi17WdF4w/HWWmrbDbWhLERdUs6ZlAqlSssbNC4Ys75HAw0if/FQG9g15zBGc4j2s1orUp+D73qs5/PVxs9Jt6+3zY9iiP2A94HD4YbXr98QY8fh9hXDdgfOYkJADIwp8fjxkSqVXHR15GWhpIVSCtM8kkvmcpl5Oo2IGLzXGAIVMZklzfzD3/03vv/DH3j35g2uGm5vDnzz+i0Yj6FinceIoesGvI/kUjBcWJZMyYWUCkvKfPx4z8PDkXG6UAWs8/gQiV3fEJz685rTdYEYI1hjcNFjrS4o5wwihZwXLuO5rXaLVHuNhc5BjIGui19v/LqiHeMwxmGswfuIOM9mu+PV3R1d13P7+g3DdkcWYa6VXCvHaebpfCGXwrQslFpZpok0z+SSOV/OpJyuxsdY+n5HCBGoYDQ5+uOf/swffv8H8pL59btvMRimJYGxYAxWLNUIocHYXArLXFrABGkX4XQ6c//wSKlJobJ1GhNCoMrqswVI6s543u3eOUJ011WvQT6zNKRleDby6ue994QQvt74K741xiHGIwaK05NccFxyJZlEPj4RpkQqlSlncql8ejxyPJ+pIqSsXEtJiZwSpWamdhGWOVNSVUgKOGNbZjpTlhkrgreKdnQlZ6Zp5nS5aNLjpGFv054tsYsYZ/EhEEIgxJntfuA89iyLUEpByHhv6fpILpV5ztegSZXrQ148MJoH2BZMrTFglBNag6xzXhGXcw1RfaXxvW+/bAPFdgiGbCIVuBD4NGfMUkjHkVKFZSmM40zOhcfjidNlbInM6p0rlnq9IFUqqRnUe48DorfMUyFdzqR5xEqh9w4HpHlmGkcej0fef/xIjIHdfiB4j7F64ayzbHauoVH1xfM083B6JNeF01NlHhNVFvreE82OeUlMy0UTxNqMXwRZH6ZSs8VaMB6ssThj8dZRpJLRjNg5R4yx0RDxCme/yvjXIGQsgm3nZCkipCJMS8IYw7RkNeJSuFzU+OfzyHgZNUg3Ms4awaKBKtdyNX7OBUQoOeujJHJO1Jyx6LZ31iqkK5W0JMZpokql63VrO9fcAkaz7/WCG+V+QnD44PF+xewV0y6WkmfPMPHlal+/x0oytri6vre1FlPMFVpev3dNhb+wuL/0wxB2ACzFsiyWXArHcWROCe8q3ldNUUSoQMmVNCsZNY4T8zQ/Gx+hSkaqYugpLVcUsqSEdY6n8ye6XgmzWjNSKjF0vHn1hsN2izMgJXH/6QP/7A2xj9zd3RJjZLvdstvtcN7SdaFtezWKdYYQHX3vmS9gZcGUiVItORnSXMipoaE5USaFmiUnRArWgniLCKSUKFnRVIwREXDOUOqzsUspjbb4CcSa90MzvpByZUmF49PEOM1Inaj1AkYTFGONZpy5IFVI80JJGRCs0SWTy0IuM6UUxnnS4JgSS1owxnC6bPEx4F0ghog1lm2I7OLA0EWcMUgpPB0fkJrouo60LHR9T84F5z0h+GZ4iwDGGow1BG+JwRIcWEmYulDFUMSQkxo/50pOiTwvyuHXAhRcfYahJRdK44SCV2rFWjW+8vjlegF+mvFbtDZLail0Js0z8zSCLBhZrshATOOAUkJqJadMzblt24IgpLyQiiKEJSXKipOlQsPd1jlciMR+h7OW3nl6awnWkNKClMQcXcsyhXEcKbUSQsCHFWEIpWjBw9rYFpKj7yKboedmv2WOlvNcGFNFqqELmhfYELAxAkVzBQo+eJyzVx6n1tIu7or79YLXWrF25XWkXbyvNP5ms0NEOI1H5nlinCaeHh44nU44W/AmIVSWNJNLIueFNJ+V22hGLaUyN6g558ySlQKgJSyx7+j6XnOJGAjdwLA5cLh5R/CeG2/ZOMtyeeL04c/UkpCaqHUhxI5SKz5ETqczn+4f6LrIu3dv2e027HZbXUBi2PQ9ctgx2MRGfskyT3y4P3F/vDB6zVaXUMkeSnQIBZEFhb3ScsTCsowsy6xQ1TuMdcQuEkOklIIWc+qV7/lq4xtrr4ghl0zOiZQW0jJTTcG4gkghLTOpLJQ8s8wjImU9X+VcllnhXM7MOYMx7cQtQQTrHNY5vA/4EAmxJ3bK8UdviNZQ5pGcE3mZWebIMgdEYJpnQpVrharWyjzPhODp+/669Z2zBO8genLf4Y3Qx5nOO8q68qnYGsg1ITTWkoKgC0nEUKtoaREDxWBlhePms4eu/Pr1xn94+BMiwqeHBz4+fGIaJ47HD5zPZ2zN2JoAQYyAEbyH273WbXebLUM/kHLhdFnIpXKeM+c5YayjG3q8d2z3O/aHAy4EtodbQjdgXMT6AYvBlwnqQkqZp6cTy3QhlcRpvND1Pa+qoet6XoeOYbOla0EwJaV4a1u4CtUK8/nCpz//WWEsgbvthm1viL1lKcLjw0ceH5QjmmfldUS0OldKbrGhQC6truFwPuJ8aC60cU21kHL6Yvnxi8a/v/8OQbi/f+DT/SemaeZ4+sB4HpGUkGXBYOh69bfdtuNmv6frAu/evuPu9o4lVY6nxJIrx0viOCac8+wO+1apOnDz6g4fAt12hw8dcyqcxoVaCu5yD3Mip8Lp6cR0OTHOE/4UGIYNxnVsNoXbu1dshg0heBC0bJiVdwGQoq5wvpy5//7PpGnk9s233N0cKMazNQNZwMrENN6zLMJlVFeqbqRe417J+nXK6vu7fkPsK1XqNQauLOtXr/zz6ajZ7DJdkYMPgdhVcA4xCuc224Gui+y2PTc3B7ousNns6LsNxlZiSvqcF0JWUs65iHOhPUec9VgTsNbrbhKjhvsMb0sj4ArFgHWJeV5wLjQm0eN9IIROcb3XbFMxe9V67JKZzheWaWTYXOjjBgkdfhiwxtF3gb6PGCv40VGqvfrwlac3RhMu77W4b6y5Em3r62otP83t/OPf/18AJLfBuA2h77h99YZSBFcqLle8c7x+fcduv2G3G/jm7Y0iDxvxJjAtFUxiSZXqRoqdwFp8jFjnsW6DtQPGekQCpXjF/nOm5oJJBVuUDa1VKFWYpoU0w5gF3x0Zl8xvlkzf9fTDwN3dDX3fEWPAGKfk3lJIl4XzwxPv//k7pvOJMmXSeaQ73PDqd3v80JHyFuPecBkv5DJjz3AqyjNVeVFEauoKY0xDWOqW5nmklMKyLOS8fL3xj4/3Sl5tLHa3xeKIfQQxuCqELITgub2943CzZ7cbePXqBu89NRmkGCqVEBNiKiGBQmODtUEzaOMxxgOuZdGGWg2liLKLK6+yZp8ipFqZpWJsYl4WnPOUtvKDD3RdzzAMWHfVuVyz4zwnptOF8XxmOp0ZY4frOpwVQjAMfWC768FUYheYF909pVHHmkk7rLPK41iDa8UiRTrlCkdXOvqrjD+eJjCG7eAYhhuM9RgiBkcQ6ETw3rG/uWG7GwjRs6RKzonpUkhTZcnCZRRy0QTEoDDIGsFYLTyLaPCqUjBiWnar2bA0tssYi48doSEmKQVxEWxQGtrYlv2/1AmtehuDs57gIn3s2W12eKl0weMtWCqGjCWzGQI27tlsorKv5xu6GFmW1EqFQi3grKfvN0pl6J8HYLPZXFd+SssX67hf9vmPF4w1bF85tts7nO/wfos1gWhgY8E7y+GmZxgiUjNLWqil8ng/cj7OVLHk6qgYkiQtWmMwtipraipKTQlUrZnWmhTP54yoBKxt94FcBGsWLWS7DuOiPqz77xetBYwYvPVE3zF0Azf7A72FPka8EbytWJOwZmG3C9x0A8tS6LuBaU5453h6ujDPC09PWnp0TuOacxYxBTEF7z3euyvcXZafQCkrUm+8ubFNXLRWfrgGG9O4dTEGBRXyOQ9l1ncyWF32V9XZtQ4qglBB7LOOo+E0oQmTnNfit6uYImDd8+OZOtX3ZN3yqmIz12zUqvzk+mhUtLRMuz2sEYJ31CoMfcduu8E7T0oCkggh4vxzxU6JumdizTmHd18275eJtW6DMZbgIg4wtbLkEZEJiQFnI1Usi1RcFUpRjaJUg3hPGJQtrKKpuSmCWRMit27VSq4JawpObDNVxhjlVTDKqhoficOOagPeTjhmXOywYYMNPfiIOIs4EJMRkqoPqsE0VcJKX9jocNnjO0foPM4bpC7UbJmLIFMFLMEGfOf4xTeviaFjmhJ//uGRp6cRrMUYr7CyQrlW0IEqBOfxvf96nO99bMHRqVFEKCVRKngHRdyV1cytgp/b4sE6XNRqkm0LuRpLLbbthrZSUViGEawUzRxNxdA0NavxrcOFHi8GlwWbRQVb68O6K3ctpoIpNPkcqwWM0Z1qnVLJzlusM8paSqGWTJFElYwxWr2yznHYbgheXdCSHc6PpFxYktLikmcFC9dqjFbK1r39VcbvN1td+TFeI4pGcyFly5QyXqDLggmKUnK1zdCGaow6gNpquFqjaIZvPLpxiLWIdVjfyno503UOyYJnVZDxWcFa47bWlp1T9KHeb1XZVcQ8uzRWha9V9ygGStWkyZcWV3ipmRetN1Oo1WKNI3jL3WFLjB3znDhdxhaUHTkrOtOsumKN+2kF9MPdK8DQD5uWSND0jwUBirH4UvFJqMEg1VKqQ0SdB07rwLkuitEFipY7ruqBaj3FeIz1uNgRu6joIwck06R9Bete8CpVg6gx9lorVQio1pPm80UqFU35jQHj1PXoBYBcCiwVl2JDVRWLKuxqFZZlVHrCdXjf44PlF9+8ohrH6XThw/0D87yQlxPTWKmlkGZlgJ31WPcTyojWeQ1UqxZTVE4idc3i6lUpVhoeL6uMQrGjciJ1pY6fy0BrYLL2+dlZi3MWnKV6rZ5J+7drruIaKI3FroG/rVSuz2vAfwE7r5WmleNvn4kX5yutAL6+jtp+vWKkgjV4Z8BaYnR0UZUWMXqC95RcWAHEv0a19qNoR4zRWnJRA6eUWJZENAYfPFIhpRk7m7btsl6ElKitPEjVk3LW4p3BWeg7pwXuPhI3Aec9uyHQdYHqotaMi6P4SpkNRgbOt3vGLiLugmDoYsBJQUpqOUFB6jPkFLNKdEGMAWdwwdP3A6YmMAVjGjffKAjnNWOtFRyOWjSO5TyDMdSSwVo6Z3j3ekMuPZ0Xbg87Hh4emS4jkifNC9L0pXj7r4Ga6m5qyxBLzpSUKM41mrVSciInqy6mGT9NMzklDChSMmCDBjBnLV10umI6Tx8DLji20ROjQ6xHTESKJUkmW6HmyG474JxlSoV5zgTnsFSouSnhiu7Mhm/1AjQZiNWTsN4RukDNERFlHW2jgKWqHNI7h1iw4lQzmrImTAA2gbWE2LHfbBFReNn3A0aEP3nPbAwlZ6bpJ9ALa+Q2Uq/KA2cEbyE66D14D7te66OIgxoUFQ2BkrMGsuvKNzinesdNELyvDJ1hu1VC7LDv6LqoqzdbpBZSH8nTTN9F0lKYpgXrIiF2eO/Y7Tpi9Ox3W+WUGvZei+nP2J9r0LXOtcpTQarGjucMprkegVXBbK3WgEXWnSQYKdS8qNjLQt8FdpuBN6/v2Aw9Hz89UsoTX8KaXzZ+1cKHkYIVNWDnBOthFw03vSFGy5ubyH43qP6+6VZUUSwtCE3K8pVEzQvWWWInOF+5OThevd4QYuD27kA/9M1GTfYxjuRl4XS6cHs4ME8Lnx7OPDxecN4ybAPeO37xi9fstgMh+HYBNF5dkz2L5otNz1NjoJaKFHWHFj1na4zWio2QUdmID5bQOarAklPD9IllUrl45we60NG5O2LwTPPC3//D70k/Usf9EdGUriDXlMbVVIK3GNFidNcem86x6bVjpPNBg2hrTCgls/iGerIhJ9GCdnQ4ZxiCY9OpC9p2nr4LGhCdVpKCgeQdUoXtZsA7x5KEWrVLZNioyxr6rlHVGoyvLvPF2pM16LaAa6rGtJdq4nX1S8uUQVG2dRYjgi3ajyKoOBZUuGWbiHa31UaPzdATY/h64/+7X/8CjGF/94b9q7fUWnl6eiKlhe2m5+ZmSxcDb9+8Zr/fEYMWqJ1xVzqhlMw8jZQmxchluSq6jDVsNhv2+z3ee7aDuhBjLcbpNne14I0wj4ZaRnKa2W082/6G0AX2tztiF+iHyDB0V+pAscpKtdXnNoG1wc5ZpKg+c+2kUbjZ0JAA3mgFzGqwlqoJZbny+u2iVa1LWwN97/DB8PbtLakW5Gvdzq9++Q6D4eb1W+7evqOUwuPjhnme2W46DoeBLkbeNOP3sWO/2+FaRmykGX++tAaGRC6ZlSJa5RcxKk6PXWhdJR7rvVISNWEpmlHnmVomtpsbNsOefuh49c0rYtepslkKa7EDVoroua1pzaJs46SekWAr2BT16KvxjWv+37aHXsZrE4Y1riV/Kq/WYkxHxfHq9Y0GxK+lF/abQVf+bsNht9XMjcqydGyGTqV6wTMMPbFJN65k1UqiGYevEesLrjpczVeijdbC44J2I5qru0AZ4VYdEtGgF7uAiOLqGJUe0JVb1l9YTalr/qXU+8UaXJnmz19Tr39rfT2si0SzaVbInYqKa9su7rueEGxDVtqNY42Kqb7a+L/51TuMMbx6+y1vvv0liDDPE6VkQhfph04ZPK/ZnDXNV19XhcHhGGIA89IAckUin6UhAgWwmkpTm4Qw5wXnDHd3B3LKxNARfId1jloW0qIrXlY3YNaWn/YX1wYGrkJBZA3GDUaXlLHGUIqnisrcsVcOtxnfkKfCdE5M48TT8YSxhl/+8i2bVzdKodQFKjhbiN58vdvZbgaMMey2A/vtABj6zmvfawzEvlci6+UvmfaflkkqMfdsZfkL9AfPq6+WdfXxudEaPaDuyRKcPiskrJrDrYrh9r2XV/Ya815EX3m5KJvPr7Jm4St3xOfkWFv5NTe96HnEWENJpbGxz1S4QfgRaufHFWvql71CSGtwVreo9R7nFdO8LJatPL1dO7/Xs5a2Cq48vxJZV7u8cA0VNfZLmxnrCLHH1oJtrZztI9KoT9bE6vpb61WuV4COYKjWImsN4kovtN79tXIGDbGBlabPKYZQLKEY3AKMynEtTxNjd8EGR9h0BGsZbSYY+fpK1tX4IRB90z8Gi0YjA27lR9oFN8/rxLz4evWtrAzji4tw3SnQXINoY1q7QGpTg3Ge0HW4pmDQ9zFUsS2nMM/v+WIvmrVhd+UZMIixVOue3RSNAqkWI0V5HAxOlP621eDEQDb4bAjZ4GaBc0YQ5seRs/f0257Ntsd6R2cN4csu/8vGd04rRNfeJXNdx+2cW2CrzbNdyaz2wZvRa/vhv5Bcv3ihCOSaVSHQ1rOIME6jdgWKkGt5Ri5tVZr16xasG3jU75qXmP35nJTM0x37WcBd37+5OdCeMsN6vrowVgJQW0Z1l9Si0pTrhrNrT9ZX+vyu32IM+NAhrVSXihYQ9FAkrVoVuYqKVlZvZTJ1csdzpx5o0qUXRq48fcr52s+r8mzlx3Mt2lMbtdvdB226c9bThaDFHmNx1jU3UptbE9bCjBhd/s5B13lM9cgIRTJSaTKPSilJoeOV2V8DszrEGCx0nrLpSLe71hRhSMtC6FyrORuCswxd/PqA65royFj1sRU1VqnNM7cAtRYQSi7My4xUITcj1qo/f2l8vSDlepH068qyaLNcyVmFWu3iVBFVxG0GnHfE2kKaM4QVi9OmkjQsrknC82cx7aIYo7pNDdhA+34thWq5FuwFASPPe0mqZrjWaC9u9PR9VJBguIqkhGc+6NrZ8zXG/6ff/wGDod8+MGw/ICJMy6i6lJLaKllRirSeqdSGRczap3oVlmppzTbux17RypphCss8U7IKj06nJ91Jrbs8dpHtXieR+BjxIeCdZ+i2OBfo+46hV+ir1IU2K3edV4OXpBwVgrcG8ZYYPBKDwuQ2UkaA2i6SYs16LT/iIQ4BHxzGacpVS6U6ECt0m05BiDX4oH7/q43/X/7r37Ut0CG+o5bCZXwi56WphS9tRaAxq3WalFI5n8+M44iWEfVt+mFDPww47xm2G5z3OGNwxiK1slxGSko8nZ748OEHHdeyLKSc6IaB21d3msi1ZjPnPH2/xbvA4bDj5uZACIGb253KF3cbbu/2eGcYHMSm0YneYouDGLANLhvXDAwUqZgK1hWu00isop8hdFgs3SYSBpUptnoZLjp8sBgH0Xuc/AT1wvHpBBhwCfGqwhrHMzktLPPIeD6CyFVWsvr3Wiun04nL5dIKWopMigjVgPcB4x2+liYnQateuVy7AD9vndd0f1WvFSlkCtYWatEC/1q7VYlgpesjxgjDJiLe0cUmL2nB3DZ+KTiPaBRtMpjPs5CXxNxKiRhjsN7ig9fYhrpk6xppZ/RCyV+817/J+P/33/8jYAjDHr+5AYScRmpNHO8/8vH7PyIi7DYbuhhxzhNjAIHT04Wnp6e28p+hqR96EEtuTQdlnknjTHCed3ev2PUDKb/ml7/8VjtYciLnrJ3uQ48xhst4YZwmUs6cThdyzjwePyGibfe77UCMgV/88lty+Rs2fUe4O9Bvekw1rLSf7wZw2uJarUWsIXRNvmhAWkZ41eWIxdiqRvXgeocV92xfC8ZprPnXDK/7ovG//+EjAN2+0iffEpYZkcz94xN//v49iLDc3LDdbIgxgmwAmKaZcZxawFQiakhbHQkjjmo0MC45M44jfYz0Xcftzc2LFFh/Xmq5ugYR4f7xQS/mOJHSI9M0MU4jl9abO/QdIWhS+ObNa+p2S9oO0PeaooheAB86bAiK2Kxmx8bzL1LwVYJlEKpRBCXOYNGA+jxCTBCbNSlklWt9pfF/9evfgoG4fUXcvwKEWkakZvI88T5o56AOEhpaM1oAEVwbOvVfwGQAABIYSURBVGFolSxr6PqO3WFHiJHtYadyc+cIxrDpOt6+fc23r980bY3SFiknVRm0nSMiHG73XMaRZVl4+80blrQwjiOXy3jlzw3w5s1b+m6L9x0lC/O0MJ8nTk9n6jLiasGKdsxPRf32cNsz3KrINsbwLP+QtRZgW8cOrOOWrtfJSKsZaMvresG+yvj/83/6z4DBb+7wu1eIVNL8RMkzUgp//Kd/pObEbnfgsN/rBCirWkUfekJYVL0gCWMN+5s9b799Q+gim8MeHwPpMpGetuyGDf/xb/+G33z7Lc47QtSdtuTcjP/Mx5RSyI2LmdNMqYXLZeJyGUlL5vHxxDIvdJ3O2PHWkGbhuFwYH47cf/+BMl8wuWBqZloy96czWSrvfvsN37i3xC60BRVanGlyQO81OFehNtZSM+I1CXvOMaz50rr/MWJtq324dthgu17dhyxkq0HTWofYqsjDus9Ci8FccbfKT0wb8+ivglLnHeIc1evgOO8doXFG3vtnf7um6S1ldd4R0EAe+0CVqh3szrWBF8LkAsHH67iZnGZMTczTzDTOlHmGnDGlMC4Lp9OZXCu7y8g0qro4paK13heuU30+V8pbT0+HAK6ZujyTAV9vfN+2/jRduFwStWSm6UjJC8enp2sGW2tLUmrV5KpJSKBN7rM6NA6p1DRTqMyjwSWvGvnHE2kc+f3v/5Hp6UkLFi3rnZNCTeu0g9wYrcG61vJvnAEjnM8XdScCTbAJLuCtQ3Lh4w/vmR7vmY8/cPzuO0oata+qVMZl4cPTE6kUzjVzfxnpho43b18zDJ1m100zFPt47XhZE7XgtNRqjGCcXPMY535C+//q7pZx5PH81OQQR3KauZxPV22iNCpBxygu16x1hW3OqQ83Uqk5URDSZMjOMp3PnE9Hkgt89+fvmE5nDcKzNktPy8SSFrz3dH2Pc46uH+j6HussXa8X4XQ6czw+Ya1jv73RMZMiOOPItfD46YFPf/ojy+kjl/cfqHluDdtq/PdHNf6lFh6miX7oOV0Sw2agi/7a1d5vnofthahz4ProiF7zAdeeQyskfWkHfNH4yq8Il/OZhw9PbUTLkZRmjo8PpLRArYzjRaeCFB1iUaWSloWUVXdvrMr9Tqcz9x8fcN7hO9XU15S1lmotgqMapzi+QC7CvGSmecHazDgnrDH4OBF8BKsNFhjh6enI8fGI8567m9f0/aCjIuOGnHWxHI+PpMuJaVyoeaHk2kYRZOZcSVUwY6I8zYyzIPZI7Gdi0DzBWUvXq2TFenet3PXREr1V6qOLOO/YHzy73ebrjT+fj9QqvP/DP/P3/88/Mc0zj0+PzMtEnkfSeMaIUJaZR++uRRAtRK3t87QUHZa08Onj/XP1yzpubm54dfcKfKDaSHEdycBUFlIyPJxmzqcTOesYLxXKGqQacl44j0dSWjge73l4/EiMkV/94lfsd3v+49/+T+w3W2ob5/v7//cfqcuFfDkiJbOUQiqFLHJFOw+fJuoxYYzD//Oj1hGsEF1tczVdUzarsNdaQxdVRNv1HfubG7qu59//h8BvfvfmL5LFf4Pxa1GKd5lHzk9Hpnni6fjANE+YqkjBAikt1Px5lFmVySAt+1Tsr01lpgVsy6bfNEWvB+sR58FkFKwZctaZxiklTictxNei01rTsvB4/MiSZh6PH3l4+EAXO6KPLPPMt+++JaeF2qbKjtNITQt10fi0rvaKkDHaHJR1zgRUZNLypDeFYAsG2jTZBjm9xxpDbMbvh4GlOPq+8M0lkfJzMf/fbPzddkcV4bA/8OrmhnHqqLUQQ7gWHQw6BmztWlmv9Mp40uCnsQbXAqXzjmHYEHzg21/8kt/+7nd0Xcfd7R3DMNDFkWAj8zzxeH9PyYVpmnh4PKqkO3R435Gb0bKo1Nz4QDWG4+mJJSfef/rEh0+fMALZd4Sb1zo7wiob6Zx2qWCeZeM4p22utbDMsy7ANFKXCyJFh9+V0l5vr2VSYw19nxlToB8y736dOE9N9PDVxq+Vm/2eu5sbnfKRU4v2imPXTM40GGhb0WVtPjZt8I9Z23ecSroPB92e3/76N/zm3/8tMUS6GAnekbqR3gam8cIffVCqepp5fHxkSYnt/pbNNpIFfWCoVo1fEB7PJ54uZz58+sSH+09468ku4m9ek8eZLNot3vVDmw1qrp2FOjzDU3Li8vSJtMzM50cuTXe0LJNS3wK5FYaqaGGm6wqXFBiGwv1x4TKZL9Zxvxxwi45v8c6z3exwPpBrYZg3GCNrFbFNjH0mndrS1zTb2NbhYhU+GZ1IuN3uiDEQfdBpJSaREciWZZpZ5uX6mOeZeV7aeJhE19xhBeVZWvHcWKvcfKuczcvC0+mEd3qhjI+4CKHfUEUI63hfa/DreK4WOLViZq6koFtzmqbzlCpQaqv0OVVruNAaRGCa1E2aLyz9Lxr/8nRBgKHf8tvf/q51l+iwUZ1v1hg8s84b4yo6XQvTV3ERhjYbVX/auJsYAuPTAxYYq2AqLPPM5XRmmkY+/PCej+8/8HQ583B8JJWM6wfCRmfci3MgAeMDuKC5RNVz/HR84r/9wz8RY08/7PHbPd0guN0dImjflLXPGlMMIq2HtgpLKozTjBEhdh3OeyoVHzxz0kYIMMRuj48DGEvBMSfLn98/Ev/+n74+4K6Jkvee/a5XqZ3X3ifntKKz1kRfTtzgpfFp4imMCo6KVoVSSZRakJIpy0wRgVQwpTLPC9PlwjSNTOPINE3aWpkW8lpgEQ2Usu4oq31bUlXkimigfjwe6bqEizvCEHBYbNR6rzfacuSsJdrW7JwXSlkwxl07YRw64MIYtYWOKQPT+r5c6Ajd5tohTzWM48zjsVHyX2P8tQwmpfUvLZXz00jKiRh8G1RtdJDp+tqWcqes7TFaRm1lvdZPZSz46LHO4A34DqRUlnnS6VQpkdOiwdV7hs0G4x3ZCEWE3f6g8/zbiDARIYYeNjpXeZnGNlwapjkhOK2wlXyVjyCG0hQl1RiVKBqjmtKig5p0RqbHGe0xqKV1V9YmCmgqu9gPDNt9q+bVloXr9JOvxvkx6JS+IpWaMmlZ+Pj+B86XM30f2W6HppHfqEaxtQhJrYzTxDJrEkbOULXb0EglRM/h1QHfRdW195GaheNxYppHZCmkRef7hBDYbnd0m4Fut0UQbOixviO5xDTrTPyuG4jR62C80m7FUYVxnKnFkJaZ0oa0rvL1lHUSiTP2hfGVsCspaT4SIh6LN5CN3i2jNOMLFmu1mrY73OrKzzr2XeeDfvn4kemCK7WtrqKUzDRdOJ9PlByQmjQ41UzqYtt2mrKrq1iNr0Z3RoW/IhFkpyvfW2JwVGWMlQls0ztKYzOtc3gxdI1rwnqwqr0JIYAxOBtwtielhXmcqFnFVavyYG37ed6d7d9FJ8fW50/aiucqIAgSVL5Yn+UzCiSe8b4Pga4tvuw0Hwgx6HClr/X53jVKVBIlT0zjE99993t+eP8eZ0F7w4xWirx/IRNRJUJOus1t07ZsNwObzcDN7YG3v37DZj9w2A682W/JS2I+PlHmEZkK5+nC1Oaw+eCwLrLv1NDzUlmSqIIgqpj3cNhyuNlxuVz4u//6X/j08YM2bxttJpVcKUu+jn2XF27QGQu+qK6/8fXWOQ6Hg1Il0xPLpbSijlV34yDg8bHj9u6Gd+++0R6Etrtu9jv2u83Xr/xnPrpQayIlzXDv7z8gVW+HY4yhD9qNtw4FEhGdHFvKVXZnjeVws+cmH3BaySZ0qjrYbAeydzrWvXWlz3lhTrnVRpVHGYZBM0ZZqFknVvmuxzrHm9evePvNa56ejvzp9//M5emEbYanze6puTQtUGrGb7HIWrJRKeSqSLbW0gXtkhnrQhqbinoFFmgd13k9r91uS62FvKhJt9uB7ab/euPr2Ns2kRsddPTq9RvN7qp2ARoDscE1nSPfZCSN6TTN8MYYtruNzr887Ck4xiVzvExKyi2J05yYipBaB7vYyjQvnC4njIHH41HhIA4RR4iRzW5P6CLeudbmU7U4btfeGG3jVKTVtEZXF1OvJb9VvWbbwxhD7Hq8t9Q8s4yajO12e2IMTHPieJmUI5oujOcjtRbSsoAInqIjJL8W7SxtC5Wq6XTsen7929/y5pt32npZ9A/Zqid89a1w1VhybXg2hBgJMRD7jmw8T2NinhLH44maC6fzxJIqo4B4j+TC0+XC+w8fySkxXi6ICDeHO3a7G71rUN+x2W6J3lNz1tFgRnG7juatLYYoMlqHX9DOUaRSMCRpklupKoy1nmG7pe87kKxTE0tH33fUmrl/eOTpMlLywnh+5Hiv0pplWQBBlgt5Gr5+5bdsqSVSqvTq+17v1FMTkkMzfsW8aH7WoKhK4PXGAJo9eq1kRX9NuooRUusKqRjEujZdRGlbWpZZSiUlHYidm3b/ZVP1OlRO743FVSepid7LBgijOhzR+T5SBSttKq4xGLE6PNurkNas3A+fy0iurUQCOSntIEXbYhHIOVHyTxj5EqNid53/YQji8X1UfFz0zg4IeNEP8NyPYhCvBJWxmq6rn2wDLTDav1DBtLGOWGE4BLptRcaZFC4M08T94z3n8cISF62M1krse1xwiBXOlxO5Jro+0peOeZkIwbPZDkgp1JxbkMzkPD2PEBCh5KRj6uW5u8U6bUnabDbs9jr24HQZeXg6kZaF6XwkLxOX8cI8XgB4uv/YbkdiCV4LLTV1SKlf73a81+0ZKgQBjyH6qBLrVrlCtGqjQiSLx9OALtIGUvg2e16yToStpbQOl4LBYaxK0aPXAFXCzIzDhcBms22zH9z1hjA+RqzXHGRaRopkKgVjdWqt805vrVEyxa5eXUFDznq3o1pro5tTE+Y2F+oDxnlqVRo69h1ju7tFWnSo6zxeyG2+qAEupyM16wye7XYLPiBFc5sv2P7H+PxLC7htjouxunqNVQXBorrMqQ2jszi8dXopYlS+2ztiH3DWaBM1reuvNuxv0fbSF+oua9CYIhnnTBvLKxg2iNQm4NXB2NN4YVn07mzW0qaQ52spM5fc1Ogq/cu5MLfbKZU0a7Wu1qvxjdd7fzlnOZ1PGGuZpukKT1UYFonBY4a+TVfc0XeDSmK2O7wP7HcHtpvdl8z7IwF3+kFXYtNsqEvpMDjmtDCdZtKSuf/4yOVpxBqHt1oksV2HjVFdwE4z4U0X2Hbxuhos2sm+6XXlN2aCyyiUPFHyRIyG3a4DOsztDhEYp5FpUsPdX56oVdhPh3bnn8LcBLcpJeZ5QmolJS2gpJSZp1lX/jJfpeg1p8+MP88Xht2W8/nMPF70Pi1V6PseYqDvIrtNry2s2x193xNjx3Z70Ptzhe5Hs9wvr/yqaEekiWbMOqhLO7BrSZScmC4XTqeTuh2jfbRu6bGxjVe0VVuLTE/x5oqllfdfpyNpn6vKC7UJLjfDrHS1a2NnNICqki0tOq226xaWZdEh2lkTolxyuyBq9Osw7jQ3hYVWuZCqwzMQbNHzWm/tl9JCreWqwnBWR9L0fcd2u23G39J3zfibTbuFYFAB2dcaX8rzXXlMa8+z1WFQHNtZFUQd79/z3Z++V0Sy6DY3IWJ9YNgMvHv3hr7vePv6DvvqFlDGtNbKg9Pho4LojW2q8PD0xHfvPzAvC+8/fuR4PL5AF8LSyooVQxWnN1cQYZ5nLYifT1efvEx6E8pVxq4yl1XWAiHoHSC62MYXdAMhDsSu5/XtDd2wxdsDwSlt3nmDt0ZbYDttuo4x4v2qaOg1r2kNG1/NalILq/jOYKBWTHEYEZwUohVmyTw9fOT77/7IPM88nc6UUjHOg3UcDnvy/Bu22w2ezLbTaSXTNJGbGi2nVitOOtfgeL7w/uGBZVn4eH/P6XRqA7EXrnd3ELAu4Psd1nmWWYVPtRbmiyqpc1Ljr607sMJmrTB1MRC8p4uBw35H8J5+2NIPO50qctgTYs92M7DfbvDOsem12mbNevO1v7hTxLVm27zFF44fGfmyHiu6NX/x75c/l2v3yZps6fy0esX/yDNSvo7fuiZnzytT+6NeNCS/eKxtRfpXX7wf//K1awz57BOI/kc+M8xaiXuuxq0Qcf36Zb1Chy19Xr/Q1z6/n16Ilzb7l4f50mCGn4//sce/Rkb+8/E/6PjZ+H/F42fj/xWPn43/Vzx+Nv5f8fjZ+H/F4/8DtOLigt2uImgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 108x108 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize= (1.5, 1.5))\n",
    "plt.imshow(X_train_val[1]) #Image of a Truck\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[154, 177, 187],\n",
       "        [126, 137, 136],\n",
       "        [105, 104,  95],\n",
       "        ...,\n",
       "        [ 91,  95,  71],\n",
       "        [ 87,  90,  71],\n",
       "        [ 79,  81,  70]],\n",
       "\n",
       "       [[140, 160, 169],\n",
       "        [145, 153, 154],\n",
       "        [125, 125, 118],\n",
       "        ...,\n",
       "        [ 96,  99,  78],\n",
       "        [ 77,  80,  62],\n",
       "        [ 71,  73,  61]],\n",
       "\n",
       "       [[140, 155, 164],\n",
       "        [139, 146, 149],\n",
       "        [115, 115, 112],\n",
       "        ...,\n",
       "        [ 79,  82,  64],\n",
       "        [ 68,  70,  55],\n",
       "        [ 67,  69,  55]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[175, 167, 166],\n",
       "        [156, 154, 160],\n",
       "        [154, 160, 170],\n",
       "        ...,\n",
       "        [ 42,  34,  36],\n",
       "        [ 61,  53,  57],\n",
       "        [ 93,  83,  91]],\n",
       "\n",
       "       [[165, 154, 128],\n",
       "        [156, 152, 130],\n",
       "        [159, 161, 142],\n",
       "        ...,\n",
       "        [103,  93,  96],\n",
       "        [123, 114, 120],\n",
       "        [131, 121, 131]],\n",
       "\n",
       "       [[163, 148, 120],\n",
       "        [158, 148, 122],\n",
       "        [163, 156, 133],\n",
       "        ...,\n",
       "        [143, 133, 139],\n",
       "        [143, 134, 142],\n",
       "        [143, 133, 144]]], dtype=uint8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_val[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_val.max() #confirm that the max value is 255, as with all RGB images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_val.min() #confirm that the min value is 0, as with all RGB images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_val[1].shape #32 x 32 pixel RGB image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABYAAABfCAYAAADs4zefAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAEt0lEQVRoge2ZwY7cRBCGv6q2Pd7JbkggIkLABRROXDlxQuIVeDZOvAIXXgGJCxJXuKAggSCJFG0yO7Ox213FoT2zA0psk96ES1qybHnX3/xdrqqucou78yqGvhLqG/BrAVdTf7S/7jnAw7Tl19hybmu+v/iEnzd3Afj286/lRc8uUpxewiUnFXceSe7Eo3tBjEoN8xeKnQfvLCPjkWDF8yHTs5gEq8gIg1oSrURqSazCMAmdBa+lAcC0A3pqMe7WT3iyOikD1xIAWElFKz3GwFm45Cw8KwP/PlwAsDHlsa3Z2ooH8Rab1JaBf4lvkVw5tzWPhpt0VvMg3uQ8Fprix91HADwebvC4v0Fngaf9CduhKQN/892XAIROCM9ADCTlMwBfvCT4vR/SCDaq7QC2PAKnvWIzIO5ol9CLHjEDEVymo24W3PxxDoBcdvh2B24QAqKhDCx9DmmPEe97cEcqhzBvkkkw45RFFZomK64bJMwnxUVgQoAq/6s0NVSFpujfv404SDKkS4g7VgesKlT88LN1VjmARkccXMG10Cu62z6CBY0jTGAmx8+Dq0+f4g7DEOj7gO+JC+JkEvzVxz8B8HRoOY9roiuDBQYvtHH0cDjvob2F2fVuFnx/9w4AF3HFJq5IpkTTcvBFXAGwHRp2sSaZkkwwKzTFPu/uYs2ua3DATDErVLzpsuIuVnR9hbvgJlfe8bLgmPKUB1MsaQa7sKQwmjbFZVachkCKCqPiYj8ehuxuNig+aAZeB9ge5WVeDDSvUsh1gM/uZxu7APv35TBTts2Dq50fwPuMJu7lim/+los/V/CwPAHNgts/c4nlleJBQQXMy00hcXxjyaEe77mzxJGnwbtcVYqOikUytBTMkA6wQxAv7EemwQfQqFTkauUuAu/V2ZHzXgt4Xz+EgFchQ4PiWpiPvc1JyFcV1tZ4ULzR8rpir9jrkI9KSKtAakoVj8o8KF4JroJVgtWFif4fYBUsZGgxWIbcE6gaHoVggvWKa2GA6JPtqDygTY2rEnY1aVVYbfKsy8qrCoaUQ9sdhvm4mrbx7jJf7OtjFSQltC4Ep80mX4giKvnc1Egp+BDSnnItIY6ka4i8pXnheeP/+dgk4TlupYqUNpDSjM242dXNEPLaVwJmr1iuEr1I9o4isL59K18kgzQurKqLXup0gJzmdgyzvP79hzEdIGc50UtyGCwv/VC+Sm8/OAEHTY72ubSS6yhYhnb87jYIGjJQzJEFVpnupc/GQjBl+NK6bRYcj8B7lbmXLgSnMT7E8pHLWfD5PD/TpN8ZI87HQ8Bag9qmHpsHW/svgDhykqiawo+m9a2jb5hjPm7byLqJL35oCfjDO/krVjJlMEXFOW06TuuuDHyj7oHcQEYLKM666mlDoeJ7pw9JKL1VXIx99c2q4yT0ZeB3mw3JlZ01VKMjn4aOdSl4pXnKhnApuQmpNVEviOnpBlKzV5grO21IKLVcA7iWAUNRsbyL4DbuKBQGSCMJuNpNQKDVSCuFXrGfsoofrpeaYlFdETDCOH2VfB1mzDFfhB3/gNhho2VuLAYfv7Dil3c8bMzuASMsWEoWgRNK2oPFqGU+bb6yolDe7Eu/Ab8+8N8YLi4lWdSsFAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 108x108 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train_val[1][30]\n",
    "\n",
    "plt.figure(figsize= (1.5, 1.5))\n",
    "plt.imshow(X_train_val[1][31]) #Slice of the Truck above (there are 32 slices)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_val[1][31].shape #Each slide has an RGB column vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6],\n",
       "       [9],\n",
       "       [9],\n",
       "       [4],\n",
       "       [1],\n",
       "       [1],\n",
       "       [2],\n",
       "       [7],\n",
       "       [8],\n",
       "       [3],\n",
       "       [4],\n",
       "       [7],\n",
       "       [7],\n",
       "       [2],\n",
       "       [9],\n",
       "       [9],\n",
       "       [9],\n",
       "       [3],\n",
       "       [2],\n",
       "       [6]], dtype=uint8)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_val[:20] #y values contain classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data into Train, Validation and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val = X_train_val[5000:], X_train_val[:5000]\n",
    "y_train, y_val = y_train_val[5000:], y_train_val[:5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Neural Network \n",
    "The first model will be larger than required but will provide a few good learning points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               307300    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 500,210\n",
      "Trainable params: 500,210\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "He_ELU_layer = partial(keras.layers.Dense,  kernel_initializer= 'he_normal', activation= 'elu')\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape= [32, 32, 3]))\n",
    "for layer in range(20):\n",
    "    model.add(He_ELU_layer(100))\n",
    "\n",
    "model.add(keras.layers.Dense(10, activation= 'softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile Model - Callbacks, Optimizers, Loss, Metrics\n",
    "First, create callbacks Tensor board, EarlyStopping and ModelCheckpoint\n",
    "\n",
    "We will then use Nadam as the optimizer, where we will have to find the right learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Early stopping and Model checkpoint Callbacks are easy to implement\n",
    "early_stoppping_cb= keras.callbacks.EarlyStopping(patience=10)\n",
    "model_checkpoint_cb= keras.callbacks.ModelCheckpoint('CIFA10_model.h5', save_best_only=True) #save the best model\n",
    "\n",
    "run_index=6 #change values of the run index so we can plot all learning curves on tensorboard \n",
    "#create log directory\n",
    "run_log = os.path.join(os.curdir, 'CIFAR10_data_logs', \"run_{:03d}\".format(run_index))\n",
    "\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_log) \n",
    "callbacks = [early_stoppping_cb, model_checkpoint_cb, tensorboard_cb]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's find the right learning rate for Nadam using the tensorboard learning rate plot for the first 10 epochs.\n",
    "\n",
    "__Fire up the tensorboard__ by first setting the file path to the Jupyter notebook's directory then through the command line load tensorboard,  with the flag --logdir=< The location of your log files > and --port=6006.\n",
    "\n",
    "I used Tensorboard to find the learning rate used (below). My previous Runs has learning rates 0.001, 0.003, 0.0001, 0.0003, 0.00001, 0.00003,  \n",
    "\n",
    "__note__: If you have loaded a previous tensorboard server before - through Jupyter notebook OR command line - ensure that this is closed. You can make sure by closing the notebook (plus powershell) and starting it again. I learnt this the hard way. You can also see the location of the current working directory that the tensorboard is using on the bottom left, ensure this the location of where you are saving your logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer =  keras.optimizers.Nadam(learning_rate= 5e-5)\n",
    "\n",
    "model.compile(optimizer= optimizer,\n",
    "             loss= 'sparse_categorical_crossentropy',\n",
    "             metrics= ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train ELU Model with Epochs, Validation Data, Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "   2/1407 [..............................] - ETA: 31:44 - loss: 133.4806 - accuracy: 0.1250WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (1.409046). Check your callbacks.\n",
      "WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.198579). Check your callbacks.\n",
      "1407/1407 [==============================] - 29s 21ms/step - loss: 4.0040 - accuracy: 0.1692 - val_loss: 2.2040 - val_accuracy: 0.2220\n",
      "Epoch 2/10\n",
      "1407/1407 [==============================] - 26s 18ms/step - loss: 2.0494 - accuracy: 0.2545 - val_loss: 1.9715 - val_accuracy: 0.2738\n",
      "Epoch 3/10\n",
      "1407/1407 [==============================] - 29s 20ms/step - loss: 1.9249 - accuracy: 0.2970 - val_loss: 1.9274 - val_accuracy: 0.2976\n",
      "Epoch 4/10\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.8480 - accuracy: 0.3286 - val_loss: 1.9271 - val_accuracy: 0.3064\n",
      "Epoch 5/10\n",
      "1407/1407 [==============================] - 23s 17ms/step - loss: 1.7850 - accuracy: 0.3526 - val_loss: 1.8405 - val_accuracy: 0.3282\n",
      "Epoch 6/10\n",
      "1407/1407 [==============================] - 26s 18ms/step - loss: 1.7365 - accuracy: 0.3735 - val_loss: 1.7234 - val_accuracy: 0.3660\n",
      "Epoch 7/10\n",
      "1407/1407 [==============================] - 23s 17ms/step - loss: 1.6839 - accuracy: 0.3920 - val_loss: 1.6635 - val_accuracy: 0.3906\n",
      "Epoch 8/10\n",
      "1407/1407 [==============================] - 23s 17ms/step - loss: 1.6457 - accuracy: 0.4091 - val_loss: 1.6351 - val_accuracy: 0.4064\n",
      "Epoch 9/10\n",
      "1407/1407 [==============================] - 23s 16ms/step - loss: 1.6131 - accuracy: 0.4182 - val_loss: 1.6440 - val_accuracy: 0.4094\n",
      "Epoch 10/10\n",
      "1407/1407 [==============================] - 23s 16ms/step - loss: 1.5870 - accuracy: 0.4295 - val_loss: 1.6597 - val_accuracy: 0.3970\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2890235bc40>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs= 10,\n",
    "                   validation_data=(X_val, y_val),\n",
    "                    callbacks= [callbacks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 1s 4ms/step - loss: 1.6351 - accuracy: 0.4064\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.6350537538528442, 0.40639999508857727]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load best model \n",
    "model = keras.models.load_model('CIFA10_model.h5')\n",
    "model.evaluate(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I started training the above before dinner and the accuracy had not improved much, considering the long training time length there was no point continuing training. This model is clearly not great for our dataset. We should try adding Batch Normalization. \n",
    "\n",
    "Note, I will now save the logs to a new directory called RunNB. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELU with Batch Normalization \n",
    "BN with ELU will help reduce wall time so let's add this into the layers.\n",
    "I will add the code in one cell for convenience. \n",
    "\n",
    "There is debate about implementing BN before the Dense layer or activation function, to save a bit of typing let's try before and after. The code looks more beautiful to me. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "   2/1407 [..............................] - ETA: 2:29:05 - loss: 3.1042 - accuracy: 0.1562WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (6.354290). Check your callbacks.\n",
      "1407/1407 [==============================] - 54s 39ms/step - loss: 2.1339 - accuracy: 0.2428 - val_loss: 1.8305 - val_accuracy: 0.3358\n",
      "Epoch 2/10\n",
      "1407/1407 [==============================] - 40s 29ms/step - loss: 1.8540 - accuracy: 0.3354 - val_loss: 1.7101 - val_accuracy: 0.3798\n",
      "Epoch 3/10\n",
      "1407/1407 [==============================] - 40s 29ms/step - loss: 1.7573 - accuracy: 0.3717 - val_loss: 1.6322 - val_accuracy: 0.4084\n",
      "Epoch 4/10\n",
      "1407/1407 [==============================] - 41s 29ms/step - loss: 1.6908 - accuracy: 0.3953 - val_loss: 1.5784 - val_accuracy: 0.4340\n",
      "Epoch 5/10\n",
      "1407/1407 [==============================] - 40s 28ms/step - loss: 1.6394 - accuracy: 0.4166 - val_loss: 1.5385 - val_accuracy: 0.4512\n",
      "Epoch 6/10\n",
      "1407/1407 [==============================] - 39s 28ms/step - loss: 1.5962 - accuracy: 0.4320 - val_loss: 1.4991 - val_accuracy: 0.4610\n",
      "Epoch 7/10\n",
      "1407/1407 [==============================] - 39s 28ms/step - loss: 1.5567 - accuracy: 0.4464 - val_loss: 1.4941 - val_accuracy: 0.4700\n",
      "Epoch 8/10\n",
      "1407/1407 [==============================] - 41s 29ms/step - loss: 1.5217 - accuracy: 0.4586 - val_loss: 1.4720 - val_accuracy: 0.4692\n",
      "Epoch 9/10\n",
      "1407/1407 [==============================] - 43s 30ms/step - loss: 1.4924 - accuracy: 0.4714 - val_loss: 1.4368 - val_accuracy: 0.4858\n",
      "Epoch 10/10\n",
      "1407/1407 [==============================] - 41s 29ms/step - loss: 1.4671 - accuracy: 0.4788 - val_loss: 1.4184 - val_accuracy: 0.4924\n",
      "Epoch 1/10\n",
      "   2/1407 [..............................] - ETA: 2:45:16 - loss: 1.4163 - accuracy: 0.4844WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (7.047533). Check your callbacks.\n",
      "1407/1407 [==============================] - 54s 38ms/step - loss: 1.5422 - accuracy: 0.4528 - val_loss: 1.5112 - val_accuracy: 0.4664\n",
      "Epoch 2/10\n",
      "1407/1407 [==============================] - 40s 28ms/step - loss: 1.5029 - accuracy: 0.4669 - val_loss: 1.5613 - val_accuracy: 0.4408\n",
      "Epoch 3/10\n",
      "1407/1407 [==============================] - 40s 28ms/step - loss: 1.4671 - accuracy: 0.4804 - val_loss: 1.4306 - val_accuracy: 0.4848\n",
      "Epoch 4/10\n",
      "1407/1407 [==============================] - 41s 29ms/step - loss: 1.4387 - accuracy: 0.4895 - val_loss: 1.4116 - val_accuracy: 0.4996\n",
      "Epoch 5/10\n",
      "1407/1407 [==============================] - 40s 29ms/step - loss: 1.4077 - accuracy: 0.5040 - val_loss: 1.4103 - val_accuracy: 0.5042\n",
      "Epoch 6/10\n",
      "1407/1407 [==============================] - 44s 31ms/step - loss: 1.3843 - accuracy: 0.5101 - val_loss: 1.3791 - val_accuracy: 0.5076\n",
      "Epoch 7/10\n",
      "1407/1407 [==============================] - 40s 29ms/step - loss: 1.3573 - accuracy: 0.5216 - val_loss: 1.3861 - val_accuracy: 0.5118\n",
      "Epoch 8/10\n",
      "1407/1407 [==============================] - 40s 29ms/step - loss: 1.3372 - accuracy: 0.5263 - val_loss: 1.3650 - val_accuracy: 0.5146\n",
      "Epoch 9/10\n",
      "1407/1407 [==============================] - 39s 28ms/step - loss: 1.3220 - accuracy: 0.5306 - val_loss: 1.3475 - val_accuracy: 0.5200\n",
      "Epoch 10/10\n",
      "1407/1407 [==============================] - 39s 28ms/step - loss: 1.2992 - accuracy: 0.5402 - val_loss: 1.3475 - val_accuracy: 0.5254\n",
      "Epoch 1/10\n",
      "   2/1407 [..............................] - ETA: 2:50:44 - loss: 1.1518 - accuracy: 0.5781WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (7.279200). Check your callbacks.\n",
      "1407/1407 [==============================] - 54s 39ms/step - loss: 1.3443 - accuracy: 0.5280 - val_loss: 1.4107 - val_accuracy: 0.5024\n",
      "Epoch 2/10\n",
      "1407/1407 [==============================] - 41s 29ms/step - loss: 1.3272 - accuracy: 0.5312 - val_loss: 1.3764 - val_accuracy: 0.5194\n",
      "Epoch 3/10\n",
      "1407/1407 [==============================] - 42s 30ms/step - loss: 1.3088 - accuracy: 0.5388 - val_loss: 1.4118 - val_accuracy: 0.4980\n",
      "Epoch 4/10\n",
      "1407/1407 [==============================] - 39s 28ms/step - loss: 1.2943 - accuracy: 0.5426 - val_loss: 1.3985 - val_accuracy: 0.5168\n",
      "Epoch 5/10\n",
      "1407/1407 [==============================] - 38s 27ms/step - loss: 1.2722 - accuracy: 0.5543 - val_loss: 1.3712 - val_accuracy: 0.5108\n",
      "Epoch 6/10\n",
      "1407/1407 [==============================] - 39s 27ms/step - loss: 1.2555 - accuracy: 0.5561 - val_loss: 1.3858 - val_accuracy: 0.5092\n",
      "Epoch 7/10\n",
      "1407/1407 [==============================] - 39s 28ms/step - loss: 1.2383 - accuracy: 0.5628 - val_loss: 1.3632 - val_accuracy: 0.5192\n",
      "Epoch 8/10\n",
      "1407/1407 [==============================] - 38s 27ms/step - loss: 1.2127 - accuracy: 0.5742 - val_loss: 1.3790 - val_accuracy: 0.5238\n",
      "Epoch 9/10\n",
      "1407/1407 [==============================] - 39s 28ms/step - loss: 1.2107 - accuracy: 0.5770 - val_loss: 1.3457 - val_accuracy: 0.5386\n",
      "Epoch 10/10\n",
      "1407/1407 [==============================] - 39s 28ms/step - loss: 1.1900 - accuracy: 0.5835 - val_loss: 1.3479 - val_accuracy: 0.5332\n",
      "Epoch 1/10\n",
      "   2/1407 [..............................] - ETA: 3:01:03 - loss: 1.0124 - accuracy: 0.6406WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (7.720670). Check your callbacks.\n",
      "1407/1407 [==============================] - 55s 39ms/step - loss: 1.3084 - accuracy: 0.5442 - val_loss: 1.4856 - val_accuracy: 0.4994\n",
      "Epoch 2/10\n",
      "1407/1407 [==============================] - 38s 27ms/step - loss: 1.2989 - accuracy: 0.5470 - val_loss: 1.4728 - val_accuracy: 0.4908\n",
      "Epoch 3/10\n",
      "1407/1407 [==============================] - 38s 27ms/step - loss: 1.2807 - accuracy: 0.5539 - val_loss: 1.4014 - val_accuracy: 0.5130\n",
      "Epoch 4/10\n",
      "1407/1407 [==============================] - 38s 27ms/step - loss: 1.2723 - accuracy: 0.5548 - val_loss: 1.4430 - val_accuracy: 0.5108\n",
      "Epoch 5/10\n",
      "1407/1407 [==============================] - 38s 27ms/step - loss: 1.2542 - accuracy: 0.5647 - val_loss: 1.4395 - val_accuracy: 0.5082\n",
      "Epoch 6/10\n",
      "1407/1407 [==============================] - 38s 27ms/step - loss: 1.2411 - accuracy: 0.5651 - val_loss: 1.3946 - val_accuracy: 0.5150\n",
      "Epoch 7/10\n",
      "1407/1407 [==============================] - 39s 28ms/step - loss: 1.2175 - accuracy: 0.5730 - val_loss: 1.4139 - val_accuracy: 0.5210\n",
      "Epoch 8/10\n",
      "1407/1407 [==============================] - 41s 29ms/step - loss: 1.1979 - accuracy: 0.5845 - val_loss: 1.4004 - val_accuracy: 0.5272\n",
      "Epoch 9/10\n",
      "1407/1407 [==============================] - 38s 27ms/step - loss: 1.1883 - accuracy: 0.5903 - val_loss: 1.3666 - val_accuracy: 0.5306\n",
      "Epoch 10/10\n",
      "1407/1407 [==============================] - 38s 27ms/step - loss: 1.1726 - accuracy: 0.5925 - val_loss: 1.3626 - val_accuracy: 0.5270\n",
      "Epoch 1/10\n",
      "   2/1407 [..............................] - ETA: 2:21:01 - loss: 0.9865 - accuracy: 0.6719WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (6.011150). Check your callbacks.\n",
      "1407/1407 [==============================] - 52s 37ms/step - loss: 1.4690 - accuracy: 0.4946 - val_loss: 1.5543 - val_accuracy: 0.4826\n",
      "Epoch 2/10\n",
      "1407/1407 [==============================] - 39s 28ms/step - loss: 1.4376 - accuracy: 0.5041 - val_loss: 1.5241 - val_accuracy: 0.4744\n",
      "Epoch 3/10\n",
      "1407/1407 [==============================] - 39s 28ms/step - loss: 1.4027 - accuracy: 0.5128 - val_loss: 1.5457 - val_accuracy: 0.4752\n",
      "Epoch 4/10\n",
      "1407/1407 [==============================] - 42s 30ms/step - loss: 1.3765 - accuracy: 0.5231 - val_loss: 1.4817 - val_accuracy: 0.4978\n",
      "Epoch 5/10\n",
      "1407/1407 [==============================] - 42s 30ms/step - loss: 1.3462 - accuracy: 0.5330 - val_loss: 1.4488 - val_accuracy: 0.4956\n",
      "Epoch 6/10\n",
      "1407/1407 [==============================] - 41s 29ms/step - loss: 1.3217 - accuracy: 0.5461 - val_loss: 1.4369 - val_accuracy: 0.5056\n",
      "Epoch 7/10\n",
      "1407/1407 [==============================] - 42s 30ms/step - loss: 1.3002 - accuracy: 0.5468 - val_loss: 1.5014 - val_accuracy: 0.4858\n",
      "Epoch 8/10\n",
      "1407/1407 [==============================] - 40s 29ms/step - loss: 1.2715 - accuracy: 0.5592 - val_loss: 1.4606 - val_accuracy: 0.5098\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1407/1407 [==============================] - 39s 28ms/step - loss: 1.2589 - accuracy: 0.5647 - val_loss: 1.4005 - val_accuracy: 0.5230\n",
      "Epoch 10/10\n",
      "1407/1407 [==============================] - 39s 27ms/step - loss: 1.2354 - accuracy: 0.5729 - val_loss: 1.3931 - val_accuracy: 0.5302\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape= [32, 32, 3]))\n",
    "\n",
    "#20 hidden layers with BN before\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(He_ELU_layer(100))\n",
    "\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Dense(10, activation= 'softmax'))\n",
    "\n",
    "#callbacks\n",
    "early_stopping_cb =keras.callbacks.EarlyStopping(patience=10)\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint('CIFAR10_BN_ELU.h5', save_best_only=True)\n",
    "\n",
    "for lr in (1e-4, 3e-4, 5e-4, 1e-3, 3e-3):\n",
    "    run_nb = os.path.join('CIFAR10_data_logs', 'Run_NB_LR_{}'.format(lr))\n",
    "    #compile model \n",
    "    tensorboard_cb = keras.callbacks.TensorBoard(run_nb)\n",
    "    \n",
    "    optimizer= keras.optimizers.Nadam(learning_rate= lr)\n",
    "    \n",
    "    model.compile(loss= keras.losses.sparse_categorical_crossentropy, metrics= ['accuracy'], optimizer=optimizer)\n",
    "    \n",
    "    model.fit(X_train, y_train, epochs= 10, validation_data= (X_val, y_val), \n",
    "              callbacks= [early_stopping_cb, checkpoint_cb, tensorboard_cb])\n",
    "    keras.backend.clear_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 1s 5ms/step - loss: 1.3457 - accuracy: 0.5386\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.3457212448120117, 0.5386000275611877]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.load_model('CIFAR10_BN_ELU.h5')\n",
    "model.evaluate(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bigger gain (but still not great~52%) and Batch Normalization provided a faster convergence due to the larger learning rate.\n",
    "\n",
    "# SELU Activation\n",
    "First we need to do a bit of preprocessing, by using standardizing the inputs and using LeCun normal initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Preprocessing Step - Standardizing Features\n",
    "Before we train a network that uses the selu activation function we need the input values need to be scaled, as the values vary from 0 to 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_means = X_train.mean(axis=0)\n",
    "pixel_std = X_train.std(axis=0)\n",
    "\n",
    "X_train_scaled = (X_train - pixel_means)/pixel_std\n",
    "X_val_scaled = (X_val - pixel_means)/pixel_std\n",
    "X_test_scaled = (X_test - pixel_means)/pixel_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model \n",
    "Specify SELU as the activation function and LeCun Normal as the initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "   2/1407 [..............................] - ETA: 2:28:15 - loss: 3.1100 - accuracy: 0.0625WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (6.322376). Check your callbacks.\n",
      "1407/1407 [==============================] - 40s 28ms/step - loss: 1.9229 - accuracy: 0.3109 - val_loss: 1.8449 - val_accuracy: 0.33780s - loss: 1.9233 - accuracy: 0.\n",
      "Epoch 2/10\n",
      "1407/1407 [==============================] - 27s 19ms/step - loss: 1.7000 - accuracy: 0.3983 - val_loss: 1.7956 - val_accuracy: 0.3634\n",
      "Epoch 3/10\n",
      "1407/1407 [==============================] - 28s 20ms/step - loss: 1.6089 - accuracy: 0.4330 - val_loss: 1.7202 - val_accuracy: 0.3852\n",
      "Epoch 4/10\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.5438 - accuracy: 0.4567 - val_loss: 1.6349 - val_accuracy: 0.4362\n",
      "Epoch 5/10\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.4876 - accuracy: 0.4797 - val_loss: 1.5665 - val_accuracy: 0.4538\n",
      "Epoch 6/10\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.4426 - accuracy: 0.4986 - val_loss: 1.5138 - val_accuracy: 0.4788\n",
      "Epoch 7/10\n",
      "1407/1407 [==============================] - 27s 19ms/step - loss: 1.4007 - accuracy: 0.5110 - val_loss: 1.5594 - val_accuracy: 0.4708\n",
      "Epoch 8/10\n",
      "1407/1407 [==============================] - 30s 21ms/step - loss: 1.3592 - accuracy: 0.5281 - val_loss: 1.4522 - val_accuracy: 0.5034\n",
      "Epoch 9/10\n",
      "1407/1407 [==============================] - 26s 18ms/step - loss: 1.3293 - accuracy: 0.5382 - val_loss: 1.5132 - val_accuracy: 0.4670\n",
      "Epoch 10/10\n",
      "1407/1407 [==============================] - 26s 18ms/step - loss: 1.3043 - accuracy: 0.5502 - val_loss: 1.5231 - val_accuracy: 0.4766\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x289378c0ac0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape= [32, 32, 3]))\n",
    "\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100, activation= 'selu', kernel_initializer= 'lecun_normal'))\n",
    "\n",
    "model.add(keras.layers.Dense(10, activation= 'softmax'))\n",
    "\n",
    "\n",
    "optimizer = keras.optimizers.Nadam(lr= 7e-4)\n",
    "\n",
    "model.compile(loss= 'sparse_categorical_crossentropy', metrics= ['accuracy'], optimizer= optimizer)\n",
    "\n",
    "#callbacks\n",
    "early_stopping_cb =keras.callbacks.EarlyStopping(patience=10)\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint('CIFAR10_SELU.h5', save_best_only=True)\n",
    "\n",
    "run_selu = os.path.join('CIFAR10_data_logs', 'Run_SELU')\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_nb)\n",
    "\n",
    "model.fit(X_train_scaled, y_train, validation_data= (X_val_scaled, y_val), epochs=10,\n",
    "         callbacks= [early_stopping_cb, checkpoint_cb, tensorboard_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 1s 5ms/step - loss: 1.4522 - accuracy: 0.5034\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.4522297382354736, 0.5034000277519226]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.load_model('CIFAR10_SELU.h5')\n",
    "model.evaluate(X_val_scaled, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not much of a gain in performance when compared with the model that had the BN layers but it was better than the original elu model and it definetly did converge faster - both in terms of number of epochs and training time per epoch. \n",
    "\n",
    "#### Applying Alpha Dropout \n",
    "As we are using SELU we need to apply Alpha Dropout. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "   2/1407 [..............................] - ETA: 1:21:46 - loss: 8.7964 - accuracy: 0.1406WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (3.483204). Check your callbacks.\n",
      "1407/1407 [==============================] - 34s 24ms/step - loss: 2.1196 - accuracy: 0.2286 - val_loss: 1.8628 - val_accuracy: 0.2972\n",
      "Epoch 2/10\n",
      "1407/1407 [==============================] - 35s 25ms/step - loss: 1.8733 - accuracy: 0.3000 - val_loss: 1.8414 - val_accuracy: 0.3102\n",
      "Epoch 3/10\n",
      "1407/1407 [==============================] - 29s 20ms/step - loss: 1.7901 - accuracy: 0.3407 - val_loss: 1.9635 - val_accuracy: 0.3236\n",
      "Epoch 4/10\n",
      "1407/1407 [==============================] - 27s 19ms/step - loss: 1.7256 - accuracy: 0.3648 - val_loss: 1.7505 - val_accuracy: 0.3758\n",
      "Epoch 5/10\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.7456 - accuracy: 0.3576 - val_loss: 1.7706 - val_accuracy: 0.3842\n",
      "Epoch 6/10\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.6727 - accuracy: 0.3960 - val_loss: 1.6914 - val_accuracy: 0.4046\n",
      "Epoch 7/10\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.6171 - accuracy: 0.4197 - val_loss: 1.6802 - val_accuracy: 0.4164\n",
      "Epoch 8/10\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.5757 - accuracy: 0.4354 - val_loss: 1.6448 - val_accuracy: 0.4126\n",
      "Epoch 9/10\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.5402 - accuracy: 0.4548 - val_loss: 1.6442 - val_accuracy: 0.4330\n",
      "Epoch 10/10\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.4988 - accuracy: 0.4658 - val_loss: 1.6731 - val_accuracy: 0.4282\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x28912cfbb80>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "#build model\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape= [32, 32, 3]))\n",
    "\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100,activation='selu', kernel_initializer= 'he_normal'))\n",
    "model.add(keras.layers.AlphaDropout(rate=0.2))\n",
    "model.add(keras.layers.Dense(10, activation= 'softmax'))\n",
    "\n",
    "#compile build\n",
    "lr= 1e-3 #change this and use tensorboard to find the best learning rate\n",
    "optimizer= keras.optimizers.Nadam(learning_rate= lr)\n",
    "model.compile(optimizer= optimizer, loss= 'sparse_categorical_crossentropy', metrics= ['accuracy'])\n",
    "\n",
    "#callbacks\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=5)\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint('CIFAR10_data_ADrop.h5', save_best_only=True)\n",
    "\n",
    "alpha_dir = os.path.join('CIFAR10_data_logs', 'Run_lr={}'.format(lr))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(alpha_dir)\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=10, validation_data= (X_val_scaled, y_val),\n",
    "         callbacks= [early_stopping_cb, checkpoint_cb, tensorboard_cb])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 1s 5ms/step - loss: 1.6442 - accuracy: 0.4330\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.6441736221313477, 0.43299999833106995]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.load_model('CIFAR10_data_ADrop.h5')\n",
    "model.evaluate(X_val_scaled, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not much of an improvement with the data, we should try use MCDropout too, but I do not think this will improve the performance much. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create MCDropout class\n",
    "\n",
    "class MCAlphaDropout(keras.layers.AlphaDropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)\n",
    "\n",
    "#create a model using the Alpha Dropout model just trained\n",
    "# We create an identical model but replace all Alphadropout layers with MCAlpha Dropout\n",
    "mc_model = keras.models.Sequential([\n",
    "    MCAlphaDropout(layer.rate) if isinstance(layer, keras.layers.AlphaDropout) else layer for layer in model.layers\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               307300    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "mc_alpha_dropout (MCAlphaDro (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 500,210\n",
      "Trainable params: 500,210\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mc_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets add 2 functions that helps find the mean class prediction and using that mean returns the class for that instance\n",
    "\n",
    "def mc_dropout_predict_probas(model, X, n_samples=10):\n",
    "    Y_proba = [model.predict(X) for sample in range(n_samples)] #run the prediction n_samples, different neurons will dropout\n",
    "    return np.mean(Y_proba, axis=0)\n",
    "\n",
    "def mc_dropout_predict_class(model, X, n_samples=10):\n",
    "    Y_proba = mc_dropout_predict_probas(model, X, n_samples=10)\n",
    "    return np.argmax(Y_proba, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4326"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "y_pred = mc_dropout_predict_class(mc_model, X_val_scaled)\n",
    "np.mean(y_pred == y_val.reshape(1,-1)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy did not improve from 43.29% to 43.26%. The best model we have is Batch Normalization with ELU. \n",
    "\n",
    "1Cycle is known to speed up convergence and accuracy so we will give that a go.\n",
    "\n",
    "### 1Cycle Learning Rate Scheduling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a 1Cycle Scheduling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneCycleScheduler(keras.callbacks.Callback):\n",
    "    def __init__(self, iterations, max_rate, start_rate= None, last_iterations= None, last_rate= None):\n",
    "        self.iterations= iterations\n",
    "        self.max_rate= max_rate\n",
    "        self.start_rate= start_rate or max_rate/10\n",
    "        self.last_iterations= last_iterations or iterations//10 + 1\n",
    "        self.half_iteration= (iterations - self.last_iterations) // 2\n",
    "        self.last_rate= last_rate or self.start_rate/1000\n",
    "        self.iteration = 0\n",
    "        \n",
    "    def _interpolate(self, iter1, iter2, rate1, rate2):\n",
    "        return ((rate2 - rate1) * (self.iteration - iter1) / (iter2 - iter1) + rate1)\n",
    "    \n",
    "    def on_batch_begin(self, batch, logs):\n",
    "        if self.iteration < self.half_iteration:\n",
    "            rate = self._interpolate(0, self.half_iteration, self.start_rate, self.max_rate)\n",
    "        \n",
    "        elif self.iteration < 2 * self.half_iteration:\n",
    "            rate = self._interpolate(self.half_iteration, 2*self.half_iteration, self.max_rate, self.start_rate)\n",
    "        \n",
    "        else:\n",
    "            rate = self._interpolate(2*self.half_iteration, self.iterations, self.start_rate, self.last_rate)\n",
    "            \n",
    "            rate= max(rate, self.last_rate)\n",
    "        self.iteration+=1\n",
    "        keras.backend.set_value(self.model.optimizer.lr, rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100,\n",
    "                                 kernel_initializer=\"lecun_normal\",\n",
    "                                 activation=\"selu\"))\n",
    "\n",
    "model.add(keras.layers.AlphaDropout(rate=0.1))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.Nadam(lr= 1e-3)\n",
    "model.compile(optimizer= optimizer, loss= 'sparse_categorical_crossentropy', metrics= ['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below can be copied to apply on any  model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To plot the losses and lr-rates we need tensorflow to return these values at the end of each batch. So we use a callback.\n",
    "class ExponentialLearningRate(keras.callbacks.Callback):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "        self.rates = []\n",
    "        self.losses = []\n",
    "    def on_batch_end(self, batch, logs):\n",
    "        self.rates.append(keras.backend.get_value(self.model.optimizer.lr))\n",
    "        self.losses.append(logs[\"loss\"])\n",
    "        keras.backend.set_value(self.model.optimizer.lr, self.model.optimizer.lr * self.factor)\n",
    "\n",
    "\n",
    "# before we train the model we need to plot the best learning rates, let's define function to plot that\n",
    "def find_learning_rate(model, X, y, epochs=1, batch_size=32, min_rate= 10**-5, max_rate=10):\n",
    "    init_weights = model.get_weights()\n",
    "    iterations = len(X) //batch_size * epochs\n",
    "    factor = np.exp(np.log(max_rate/min_rate)/iterations)\n",
    "    init_lr = keras.backend.get_value(model.optimizer.lr)\n",
    "    keras.backend.set_value(model.optimizer.lr, min_rate)\n",
    "    exp_lr = ExponentialLearningRate(factor)\n",
    "    history = model.fit(X, y, epochs=epochs, batch_size= batch_size, callbacks= [exp_lr])\n",
    "    keras.backend.set_value(model.optimizer.lr, init_lr)\n",
    "    model.set_weights(init_weights)\n",
    "    return exp_lr.rates, exp_lr.losses\n",
    "\n",
    "def plot_lr_vs_loss(rates, losses):\n",
    "    plt.plot(rates, losses)\n",
    "    plt.gca().set_xscale('log')\n",
    "    plt.hlines(min(losses), min(rates), max(rates))\n",
    "    plt.axis([min(rates), max(rates), min(losses), losses[0]+min(losses)/2])\n",
    "    plt.xlabel('Learning Rate')\n",
    "    plt.ylabel('Losses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "352/352 [==============================] - 11s 30ms/step - loss: 86.8612 - accuracy: 0.1418\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rates, losses = find_learning_rate(model, X_train_scaled, y_train, epochs=1, batch_size= 128)\n",
    "plot_lr_vs_loss(rates, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100,\n",
    "                                 kernel_initializer=\"lecun_normal\",\n",
    "                                 activation=\"selu\"))\n",
    "    \n",
    "model.add(keras.layers.AlphaDropout(rate= 0.1))\n",
    "model.add(keras.layers.Dense(10, activation= 'softmax'))\n",
    "\n",
    "optimizer= keras.optimizers.Nadam(lr= 0.01)\n",
    "model.compile(optimizer= optimizer, loss= 'sparse_categorical_crossentropy', metrics= ['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1407/1407 [==============================] - 26s 18ms/step - loss: 1357660502884352.0000 - accuracy: 0.1050 - val_loss: 3.0146 - val_accuracy: 0.1038\n",
      "Epoch 2/20\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 2.5525 - accuracy: 0.1001 - val_loss: 3.7573 - val_accuracy: 0.1040\n",
      "Epoch 3/20\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 2.4269 - accuracy: 0.0993 - val_loss: 2.6997 - val_accuracy: 0.0920\n",
      "Epoch 4/20\n",
      "1407/1407 [==============================] - 28s 20ms/step - loss: 2.3238 - accuracy: 0.0996 - val_loss: 2.3070 - val_accuracy: 0.1010\n",
      "Epoch 5/20\n",
      "1407/1407 [==============================] - 26s 18ms/step - loss: 2.3034 - accuracy: 0.1007 - val_loss: 2.3049 - val_accuracy: 0.1010\n",
      "Epoch 6/20\n",
      "1407/1407 [==============================] - 26s 19ms/step - loss: 2.3028 - accuracy: 0.0994 - val_loss: 2.3044 - val_accuracy: 0.1010\n",
      "Epoch 7/20\n",
      "1407/1407 [==============================] - 26s 18ms/step - loss: 2.3027 - accuracy: 0.1006 - val_loss: 2.3043 - val_accuracy: 0.1010\n",
      "Epoch 8/20\n",
      "1407/1407 [==============================] - 30s 22ms/step - loss: 2.3027 - accuracy: 0.0984 - val_loss: 2.3041 - val_accuracy: 0.1010\n",
      "Epoch 9/20\n",
      "1407/1407 [==============================] - 29s 21ms/step - loss: 2.3027 - accuracy: 0.0978 - val_loss: 2.3042 - val_accuracy: 0.1010\n",
      "Epoch 10/20\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 2.3027 - accuracy: 0.0989 - val_loss: 2.3041 - val_accuracy: 0.1010\n",
      "Epoch 11/20\n",
      "1407/1407 [==============================] - 29s 20ms/step - loss: 2.3027 - accuracy: 0.1001 - val_loss: 2.3040 - val_accuracy: 0.1010\n",
      "Epoch 12/20\n",
      "1407/1407 [==============================] - 28s 20ms/step - loss: 2.3027 - accuracy: 0.0983 - val_loss: 2.3041 - val_accuracy: 0.1010\n",
      "Epoch 13/20\n",
      "1407/1407 [==============================] - 30s 21ms/step - loss: 2.3027 - accuracy: 0.0986 - val_loss: 2.3041 - val_accuracy: 0.1010\n",
      "Epoch 14/20\n",
      "1407/1407 [==============================] - 29s 20ms/step - loss: 2.3027 - accuracy: 0.0972 - val_loss: 2.3043 - val_accuracy: 0.1010\n",
      "Epoch 15/20\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 2.3027 - accuracy: 0.0970 - val_loss: 2.3040 - val_accuracy: 0.1010\n",
      "Epoch 16/20\n",
      "1407/1407 [==============================] - 29s 21ms/step - loss: 2.3027 - accuracy: 0.0992 - val_loss: 2.3043 - val_accuracy: 0.1010\n",
      "Epoch 17/20\n",
      "1407/1407 [==============================] - 29s 20ms/step - loss: 2.3027 - accuracy: 0.0980 - val_loss: 2.3043 - val_accuracy: 0.1010\n",
      "Epoch 18/20\n",
      "1407/1407 [==============================] - 28s 20ms/step - loss: 2.3027 - accuracy: 0.0983 - val_loss: 2.3042 - val_accuracy: 0.1010\n",
      "Epoch 19/20\n",
      "1407/1407 [==============================] - 27s 19ms/step - loss: 2.3027 - accuracy: 0.1001 - val_loss: 2.3044 - val_accuracy: 0.1010\n",
      "Epoch 20/20\n",
      "1407/1407 [==============================] - 28s 20ms/step - loss: 2.3027 - accuracy: 0.0975 - val_loss: 2.3042 - val_accuracy: 0.1010\n"
     ]
    }
   ],
   "source": [
    "#call the 1Cycle class\n",
    "# number of instances/ batchsize*epochs\n",
    "onecycle= OneCycleScheduler(len(X_train_scaled)//128*15, max_rate= 0.05)\n",
    "\n",
    "history = model.fit(X_train_scaled, y_train, validation_data= (X_val_scaled, y_val),\n",
    "                   epochs=20, callbacks=[onecycle])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 1s 4ms/step - loss: 2.3042 - accuracy: 0.1010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.304230213165283, 0.10100000351667404]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_val_scaled, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is training faster than previously, we saw 36 seconds per epoch without 1Cycle and now we are seeing around ~26seconds. This is almost a 30% increase in speed and almost half as fast compared to Batch Normalization with ELU ~50seconds. \n",
    "\n",
    "Alebit all models were trained with small batchsizes, you can still identify which combination of initialization, normalization, activation function, optimizer and learning rate scheduling helps reduce the number of epochs and convergence time.\n",
    "\n",
    "The question to ask, does faster convergence time offset the increase in accuracy? In reality, and under a production environment, I would be cautious with the hyperparameter tuning and the hopefully use some sort of cloud based platform to help speed up training. \n",
    "\n",
    "## In summary\n",
    "\n",
    "For Deep Neural networks use the following configuration (<i>as a rule of thumb, this may change based on application</i>):\n",
    "<li>Kernal Initializer = <b>LeCun Initializer</b> </li>\n",
    "<li>Activation Function = <b>SELU</b></li>\n",
    "<li>Normalization = <b>None</b></li>\n",
    "<li>Regularization = <b>AlphaDropout</b></li>\n",
    "<li>Optimizer = <b>Momentum Optimization (or RM Prop or Nadam</b></li>\n",
    "<li>Learning Rate Schedule = <b>1Cycle</b></li>\n",
    "\n",
    "Don't forget to add in ModelCheckpoint and Tensorboard Callbacks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
