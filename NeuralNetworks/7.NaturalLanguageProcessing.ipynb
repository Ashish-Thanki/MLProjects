{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "7.NaturalLanguageProcessing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Hw1Pl00Na84"
      },
      "source": [
        "Alan Turing's famous Turning test helps evaulate whether a machine's intelligence matches a human's intelligence. This test was called the imitation game. Where a machine has to try fool the human into thinking it is a human. \n",
        "\n",
        "A common approach to language tasks are Recurrent Neural Networks (RNNs), but there are many other types that have other use cases:\n",
        "\n",
        "- Character RNN used to predict the next character in an sentence, using a Stateless RNN and then a Stateful RNN.\n",
        "- Sentiment Analysis by extracting a feeling within a sentence\n",
        "- Neural Machine Translation (NMT) capable of tranlating languages. \n",
        "\n",
        "We will also look at how we can boost the RNN performance by using Attention Mechanisms and Encoder-Decoder architecture, which allows the network to focus on a select part of the inputs at each time step. \n",
        "\n",
        "Finally, we will then look at a Transformer, a very succesful NLP architecture, before discussing GPT-2 and BERT. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrLup-cUU73q"
      },
      "source": [
        "import sys \n",
        "sys.version_info > (3, 5)\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "assert tf.__version__ > \"2.0\"\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYAjixdeXYNW"
      },
      "source": [
        "# Shakespeare Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gimYliCaEXP"
      },
      "source": [
        "Below is an example of how we would work with text data by converting it using a tokenizer, how to split text data because we cannot shuffle the data as we do with tabular data, "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abW18r3CVfvw"
      },
      "source": [
        "shakespeare_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "filepath = keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\n",
        "with open(filepath) as f:\n",
        "    shakespeare_text = f.read()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nczol5YSVfm0",
        "outputId": "711ecb5c-3fac-463d-b606-c0059a697af1"
      },
      "source": [
        "print(shakespeare_text[60:250])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ChbHiyu1Vfcz",
        "outputId": "d0bc9f88-2d9c-4bf8-d1e0-0a720e263411"
      },
      "source": [
        "\"\".join(sorted(set(shakespeare_text.lower()))) # list of characters within dataset"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n !$&',-.3:;?abcdefghijklmnopqrstuvwxyz\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YMmVDYDMVeB"
      },
      "source": [
        "## Tokenize Text data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOY8bdfKYGSP"
      },
      "source": [
        "# convert all characters into a unique character ID\n",
        "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
        "tokenizer.fit_on_texts(shakespeare_text)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5lJiaXRYjzv",
        "outputId": "0f94d552-3a1f-433c-f048-fdd58c97f802"
      },
      "source": [
        "tokenizer.texts_to_sequences('Romeo')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[9], [4], [15], [2], [4]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ghLYG3d5YvkV",
        "outputId": "991e3bb3-69b6-4561-c591-e18f3f2dcc72"
      },
      "source": [
        "\"\".join(tokenizer.sequences_to_texts([[9], [4], [15], [2], [4]]))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'romeo'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6g6QfaRZJ7z"
      },
      "source": [
        "max_id = len(tokenizer.word_index) # number of distinct characters\n",
        "dataset_size = tokenizer.document_count"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOy1BJcFZmHX"
      },
      "source": [
        "Note, the word encoder sets the IDs from 1 to 39 so when we convert the entire text to ID we need to subtract 1 so we can get IDs from 0 to 38.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7usNmIcIZJvA"
      },
      "source": [
        "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dx7gJR1Ntgp"
      },
      "source": [
        "Before we talk about how we can split the text data into training, validation and test set, lets first talk about how we can split the time series data.\n",
        "\n",
        "##### Splitting Time series data\n",
        "\n",
        "The safest way is to split the data up across time. For example, take the years 2000 to 2016 as the training, 2017 to 2019 as the validation and leaving 2020 to 2021 as the test set. Ensure there is no overlap in the sets. \n",
        "\n",
        "There are two problems: correlation between time series data and assuming your data is a stationary. \n",
        "- **Correlation** between variables can lead to an optimisitically biased generalization error, because the training and test set, both contain time series data which are correlated. In these scenarios we should avoid having correlated time series across the training and test set.s\n",
        "\n",
        "- Assuming that your data is a **Stationary** time series (i.e. the mean, variance and autocorrelation does not change). This assumption works well for most time series data but some time series data has disappearing patterns over time. In these scenarios we would benefit by training the data on short time spans. You can plot the model's error on the validation set, and if you observe increasing errors towards the end of the data then you know the data is not stationary enough.\n",
        "\n",
        "For example, if you have financial data for many companies, some companies are well correlated because of the sectors that they are in. Traders would exploit these correlations once they realise it, however patterns may soon disappear because of it. The correlation, alongside the unstationary nature, of the data prevents us from obtaining a generalizable model.\n",
        "\n",
        "Ultimately, how you split time series data depends on the task at hand. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Dw6iSeUaBJT"
      },
      "source": [
        "## Splitting Sequential Text data\n",
        "\n",
        "Splitting text data is pretty simple, in that we must have no overlap between the sets and introduce a gap to avoid paragraph overlapping. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLymbjaqZ8_B"
      },
      "source": [
        "train_size = dataset_size * 90 // 100 # take 90% of the data and // 100 to get steps of 100\n",
        "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFboo1mPYgl5"
      },
      "source": [
        "The `dataset` now is a single sequence of over one million characters. Recall how RNNs work on the previous notebook. If we were to train the neural network it would be equivalent to training a deep neural network with over a million layers - with only one (very long) instance!\n",
        "\n",
        "Instead, we need to convert this dataset into smaller windows of text. The length of the window size is the maximum pattern length the RNN will learn. The RNN will unrolled over the length of the substrings, this is called **Truncated Backpropagation Through Time (TBPTT)**. Read [this](https://www.quora.com/Whats-the-key-difference-between-backprop-and-truncated-backprop-through-time) Quora answer to understand the difference between backpropagation through time and truncated.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfdl7EziMaJJ"
      },
      "source": [
        "n_steps = 100\n",
        "window_length = n_steps + 1\n",
        "dataset = dataset.window(size=window_length, shift=1, drop_remainder=True)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlIYyUuDdeoJ"
      },
      "source": [
        "The `shift` argument causes the difference between the next window to be 1 character. For example, the first window will be 0 to 100 the next will be 1 to 101 etc.. Setting the `drop_remainder=True` argument makes every window size equal to `size` argument. Otherwise, the last windows will go from 100 to 1 characters in length. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ACj-8o7MaA2",
        "outputId": "450bbe56-8f0f-4fe3-af19-e7e2dae16df3"
      },
      "source": [
        "dataset # datasets within a dataset"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<WindowDataset shapes: DatasetSpec(TensorSpec(shape=(), dtype=tf.int64, name=None), TensorShape([])), types: DatasetSpec(TensorSpec(shape=(), dtype=tf.int64, name=None), TensorShape([]))>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnsHmIUtedgQ"
      },
      "source": [
        "# we now need to flatten it, as the model only accepts tensors\n",
        "dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
        "\n",
        "# the flat_map function flattens the dataset\n",
        "# the lambda function forces it to create tensors of window_size length\n",
        "\n",
        "# for example, if \n",
        "# example = {{1, 2}, {3, 4, 6, 7}, {8, 9, 10}}\n",
        "# then example.flat_map(lambda eg: eg.batch(2)), would become\n",
        "# {{1, 2}, {3, 4}, {5, 6}, {7, 8}, {9, 10}}"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xV9Kp0x0gxfB"
      },
      "source": [
        "Now that the dataset is in the right shape we can shuffle these windows so that gradient descent can have instances that are indepenedent and identically distributed across the training set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5G1i1ar9gw1p"
      },
      "source": [
        "batch_size = 32\n",
        "dataset = dataset.shuffle(10000, seed=42).batch(batch_size)\n",
        "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:])) # X, y"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "672i78RZh_cm",
        "outputId": "8d063fcf-63e2-4674-e40a-78299cde632d"
      },
      "source": [
        "z = [1, 2, 3, 4, 5]\n",
        "(z[: -1], z[1: ]) # we are trying to predict the next window size"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([1, 2, 3, 4], [2, 3, 4, 5])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sM1cnC4oiSxw"
      },
      "source": [
        "# one hot encode the dataset as there are not many unique characters ~ 39\n",
        "dataset = dataset.map(\n",
        "    lambda X_batch, y_batch: (tf.one_hot(X_batch, depth=max_id), y_batch))\n",
        "\n",
        "# calling prefetch allows later elements to be prepared while the current element is being processed\n",
        "dataset = dataset.prefetch(1)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Un36huVkxHM",
        "outputId": "de51fb3b-f9d3-4f7a-c224-0212658e3807"
      },
      "source": [
        "for X_batch, y_batch in dataset.take(1):\n",
        "  print(X_batch.shape, y_batch.shape)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(32, 100, 39) (32, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvwLLHRxOk4y"
      },
      "source": [
        "## Build Model - Char RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCE2tb78U4OF"
      },
      "source": [
        "We can train a model on all of Shakespeare's work and then use it to predict a character in a sentence. This can be used to produce novel text and is pretty fun to read about. \n",
        "\n",
        "Read this blog by Andrej Karapthy: https://karpathy.github.io/2015/05/21/rnn-effectiveness/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rQxwqu7Onrs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a6826d9-9d0c-49f3-e548-8a169009066e"
      },
      "source": [
        "model = keras.models.Sequential([\n",
        "  keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id], \n",
        "                   dropout=0.2,), # recurrent_dropout=0.2), #  prevents GPU support\n",
        "  keras.layers.GRU(128, return_sequences=True,\n",
        "                   dropout=0.2,), # recurrent_dropout=0.2), #  prevents GPU support\n",
        "  keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation='softmax'))\n",
        "])\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
        "model.fit(dataset, epochs=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "   1122/Unknown - 221s 193ms/step - loss: 2.1430"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfeGPlIhVXdX"
      },
      "source": [
        "## Make Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bbKCLynOnjJ"
      },
      "source": [
        "def preprocess(texts):\n",
        "  \"\"\"\n",
        "  Function that preprocesses text data and returns one hot encoded data. \n",
        "  \"\"\"\n",
        "  X = np.array(tokenizer.texts_to_sequence(texts) - 1)\n",
        "  return tf.one_hot(X, max_id)\n",
        "\n",
        "X_new = preprocess(['This is an exampl'])\n",
        "Y_pred = model.predict_classes(X_new)\n",
        "print(tokenizer.sequences_to_texts(Y_pred + 1)[0][-1]) # print first sentence last character"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x589o7G-bYNg"
      },
      "source": [
        "Although, this is amusing and satisifying to have predicted the next character this does not work well in practice because the model would repeat the same works over and over again.\n",
        "\n",
        "Instead, we can pick the next letter randomly which will generate diverse and interesting text. We can use the `tf.random.categorical` function, which takes in logits divided by a hyperparameter, temperature. Lower values favour high probability characters while high values will give characters an equal probability. \n",
        "\n",
        "The model is ok for small data but if we wanted to realise patterns over a large time step, you can use Stateful RNNs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hs97p2A0OnD2"
      },
      "source": [
        "# Stateful RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnkHv_8Sd53L"
      },
      "source": [
        "So far we have trained Stateless RNNs, this is where at each iteration the model starts with hidden state full of zeros and updates them at the end of each time step. It then removes them at the last time step.\n",
        "\n",
        "![Stateless vs Stateful](https://media.springernature.com/original/springer-static/image/chp%3A10.1007%2F978-3-030-27947-9_24/MediaObjects/480892_1_En_24_Fig3_HTML.png)\n",
        "\n",
        "Stateful RNNs reuse the state between batches instead of reinitializing them. This can allow the model to learn long term patterns.\n",
        "\n",
        "One thing we need to change about the input dataset when using Stateful RNNs is not to split the batches up so that one batch starts where the previous batch left off. There should be no overlap like we saw with the `windows()` function earlier. Stateful RNNs require sequential and non-overlapping input sequences.\n",
        "\n",
        "Unfortnately, this is not easy to do and requires a lot of code. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R77feIJ9IZHS"
      },
      "source": [
        "batch_size = 32\n",
        "encoded_parts = np.array_split(encoded[:train_size], batch_size) # split into 32 parts\n",
        "# len(encoded_parts) = 32 \n",
        "\n",
        "datasets = []\n",
        "n_steps = 100\n",
        "window_length = n_steps + 1\n",
        "\n",
        "for encoded_part in encoded_parts:\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(encoded_part) # dataset object\n",
        "  dataset = dataset.window(window_length, shift=n_steps,\n",
        "                           drop_remainder=True) # flatten windows\n",
        "  dataset = dataset.flat_map(lambda window: (window.batch(window_length)))\n",
        "  datasets.append(dataset)\n",
        "\n",
        "\n",
        "dataset = tf.data.Dataset.zip(tuple(datasets)).map(lambda *windows: tf.stack(windows)) # create one massive dataset\n",
        "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
        "\n",
        "dataset = dataset.map(\n",
        "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
        "dataset = dataset.prefetch(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nccJT7H7oY3A"
      },
      "source": [
        "Make sure you specify `stateful=True` and the `batch_input_shape`, this is so tensorflow can preserve a state for each input sequenence in the batch. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnygTuekjnbp"
      },
      "source": [
        "model = keras.models.Sequential([\n",
        "  keras.layers.GRU(128, return_sequences = True, stateful = True,\n",
        "                   dropout = 0.2, # recurrent_dropout=0.2,\n",
        "                   batch_input_shape = [batch_size, None, max_id]),\n",
        "  keras.layers.GRU(128, return_sequences = True, stateful = True,\n",
        "                   dropout = 0.2),\n",
        "  keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation='softmax'))\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Egh_DOkqrFf1"
      },
      "source": [
        "class ResetStatesCallback(keras.callbacks.Callback):\n",
        "  \"\"\"\n",
        "  Callback used in Stateful RNN model, to reset states at the end of each\n",
        "  epoch.\n",
        "  \"\"\"\n",
        "  def on_epoch_begin(self, epoch, logs):\n",
        "    self.model.reset_states()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boEqJ_L6rQTp"
      },
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
        "model.fit(dataset, epochs=10, callbacks=[ResetStatesCallback()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3UZ0iHfOgee"
      },
      "source": [
        "# Sentiment Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0RhicOhOuNk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MALdKjMaOuFa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nn9ohtPDOtsQ"
      },
      "source": [
        "# Bidirectional RNNs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkQQqYzUOxvG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDUqSzbROyDo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXieoFmmOyXN"
      },
      "source": [
        "# Attention Mechanisms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SWk1KrlO-al"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}