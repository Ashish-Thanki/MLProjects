{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4.Loading&ProcessingData-Tensorflow.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6Mdg00UoFxL",
        "colab_type": "text"
      },
      "source": [
        "In this notebook we will look TensorFlow's Data API, TF Record format and creating custom preprocessing layers. \n",
        "\n",
        "Some datasets will not fit in memory, fortunetly TensorFlow's Data API takes care of this. You just need to tell it where the data is and how to transform it. It will take care of the multithreading and process it as efficiently as possible. \n",
        "\n",
        "When the data is read it uses TFRecord format, an efficient and flexible binary format. SQL databases and Google's Big Query service can also be read and extensions for other open source databases are available. \n",
        "\n",
        "**TF Transform** can be used write a single preprocessing function (to transform your data) and then can be exported to be incorporated into your trained model during deployment. It can then transform new instances on the fly during deployment in production. \n",
        "\n",
        "**TF Datasets** can be used to download many common datasets, such as ImageNet. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pj8tXCpG0spx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#regular imports\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIWObbhq05l3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2331b41b-635a-4794-bdb4-df3302031ee3"
      },
      "source": [
        "data = tf.data.Dataset.range(10) #create a dataset\n",
        "data"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<RangeDataset shapes: (), types: tf.int64>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdksWnXU0_N3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "68783f6d-5efe-417f-9a4d-682bfe46080d"
      },
      "source": [
        "for i in data:\n",
        "  print(i)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(0, shape=(), dtype=int64)\n",
            "tf.Tensor(1, shape=(), dtype=int64)\n",
            "tf.Tensor(2, shape=(), dtype=int64)\n",
            "tf.Tensor(3, shape=(), dtype=int64)\n",
            "tf.Tensor(4, shape=(), dtype=int64)\n",
            "tf.Tensor(5, shape=(), dtype=int64)\n",
            "tf.Tensor(6, shape=(), dtype=int64)\n",
            "tf.Tensor(7, shape=(), dtype=int64)\n",
            "tf.Tensor(8, shape=(), dtype=int64)\n",
            "tf.Tensor(9, shape=(), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dl9WO0Ue1ELW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "a5162793-18c7-40e7-ab32-bbdf3fed7a77"
      },
      "source": [
        "dataset = data.repeat(3).batch(7) #split the data set in batches of 7 in 3 groups\n",
        "for i in dataset:\n",
        "  print(i)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int64)\n",
            "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int64)\n",
            "tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int64)\n",
            "tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int64)\n",
            "tf.Tensor([8 9], shape=(2,), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YniG2pzX1QLr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "aa45ce2f-12ca-4763-dad3-20e99f8e2449"
      },
      "source": [
        "#you can drop the last batch so that you even number of groups\n",
        "dataset =  data.repeat(3).batch(7, drop_remainder= True) #set to True\n",
        "for i in dataset:\n",
        "  print(i)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int64)\n",
            "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int64)\n",
            "tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int64)\n",
            "tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtbRJ8fQ1ruj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "c18fabec-77e2-4640-bac5-74bb4aa61d2a"
      },
      "source": [
        "#dataset methods create new datasets not modify the new so always set a variable \n",
        "#filter dataset\n",
        "\n",
        "datasets = data.filter(lambda x: x>3).take(4) #filter and take methods\n",
        "#filter by greater than 3 and take 4 values only\n",
        "for i in dataset:\n",
        "  print(i)\n",
        "\n",
        "#you can even apply a function by using map, see the documentation for all functions\n",
        "#you can unbatch your dataset (i.e. split it into several datasets)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int64)\n",
            "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int64)\n",
            "tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int64)\n",
            "tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzWdF0Ef4Q5_",
        "colab_type": "text"
      },
      "source": [
        "# Shuffling your data\n",
        "**In gradient descent**, we have seen that the more independent and identically distributed a dataset is, **the better the performance outcome**. \n",
        "\n",
        "To shuffle a dataset, you can specify the buffer size, this creates a buffer between the dataset and the call. When you call the data, you are calling the buffer which pulls data from the original dataset. Imagine a buffer size of 3, and shuffling just three cards at a time, and give your friend one card. This would not be as shuffled compared to a buffer size of 52. \n",
        "\n",
        "For large datasets, you will mostly definitely exceed the RAM you have with this approach. Instead, you can split your data across multiple files and read them simultaneously and randomly followed by a final shuffle. The Data API makes this possible!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXwKRxb52MFb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7d60ae1f-8d37-45a8-aa7e-4373767d3875"
      },
      "source": [
        "#use the California Dataset, split it into multiple files and then interleave\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "housing = fetch_california_housing()\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
        "    housing.data, housing.target.reshape(-1, 1), random_state=42)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X_train_full, y_train_full, random_state=42)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading Cal. housing from https://ndownloader.figshare.com/files/5976036 to /root/scikit_learn_data\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-C2Nu3jDAD6t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Function to split dataset\n",
        "import os\n",
        "def save_to_multiple_csv_files(data, name_prefix, header= None, n_parts=10):\n",
        "  housing_dir = os.path.join('datasets', 'housing')\n",
        "  os.makedirs(housing_dir, exist_ok= True)\n",
        "  path_format = os.path.join(housing_dir, 'california_{}_{:02d}.csv')\n",
        "  #make sure the length of the files are the same -interleaving works best\n",
        "  \n",
        "  filepaths = []\n",
        "  m= len(data)\n",
        "  for file_index, row_indices in enumerate(np.array_split(np.arange(m), n_parts)):\n",
        "    part_csv = path_format.format(name_prefix, file_index)\n",
        "    filepaths.append(part_csv)\n",
        "    with open(part_csv, 'wt', encoding= 'utf-8') as f:\n",
        "      if header is not None:\n",
        "        f.write(header)\n",
        "        f.write('\\n')\n",
        "      for row_index in row_indices:\n",
        "        f.write(','.join([repr(col) for col in data[row_index]])) #see below\n",
        "        f.write('\\n')\n",
        "  return filepaths"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2hsuIvBCDHJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7667778a-b970-4285-d030-e7d57d001ae7"
      },
      "source": [
        "','.join([repr(i) for i in range(10)])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'0,1,2,3,4,5,6,7,8,9'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Z03SWnYCFRp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#combine the X and y values for train, valid and test datasets\n",
        "train_data = np.c_[X_train, y_train]\n",
        "valid_data = np.c_[X_valid, y_valid]\n",
        "test_data = np.c_[X_test, y_test]\n",
        "\n",
        "#create header column\n",
        "head_cols = housing.feature_names + ['MedianHouseValue']\n",
        "header = ','.join(head_cols)\n",
        "header\n",
        "train_filepaths = save_to_multiple_csv_files(train_data, name_prefix='train',\n",
        "                                             header= header, n_parts=20)\n",
        "\n",
        "valid_filepaths = save_to_multiple_csv_files(valid_data, name_prefix='valid',\n",
        "                                             header= header, n_parts=20)\n",
        "\n",
        "test_filepaths = save_to_multiple_csv_files(test_data, name_prefix='test',\n",
        "                                             header= header, n_parts=20)\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDeq2D5AER-S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "f830c42e-4d81-48c3-d8c0-4bb3860e54a2"
      },
      "source": [
        "for file in train_filepaths:\n",
        "  print(file)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "datasets/housing/california_train_00.csv\n",
            "datasets/housing/california_train_01.csv\n",
            "datasets/housing/california_train_02.csv\n",
            "datasets/housing/california_train_03.csv\n",
            "datasets/housing/california_train_04.csv\n",
            "datasets/housing/california_train_05.csv\n",
            "datasets/housing/california_train_06.csv\n",
            "datasets/housing/california_train_07.csv\n",
            "datasets/housing/california_train_08.csv\n",
            "datasets/housing/california_train_09.csv\n",
            "datasets/housing/california_train_10.csv\n",
            "datasets/housing/california_train_11.csv\n",
            "datasets/housing/california_train_12.csv\n",
            "datasets/housing/california_train_13.csv\n",
            "datasets/housing/california_train_14.csv\n",
            "datasets/housing/california_train_15.csv\n",
            "datasets/housing/california_train_16.csv\n",
            "datasets/housing/california_train_17.csv\n",
            "datasets/housing/california_train_18.csv\n",
            "datasets/housing/california_train_19.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7OP53xiHQJN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "8632137e-0947-4567-eb84-e1a2bd65f0ea"
      },
      "source": [
        "pd.read_csv(train_filepaths[0]).head(10)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MedInc</th>\n",
              "      <th>HouseAge</th>\n",
              "      <th>AveRooms</th>\n",
              "      <th>AveBedrms</th>\n",
              "      <th>Population</th>\n",
              "      <th>AveOccup</th>\n",
              "      <th>Latitude</th>\n",
              "      <th>Longitude</th>\n",
              "      <th>MedianHouseValue</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3.5214</td>\n",
              "      <td>15.0</td>\n",
              "      <td>3.049945</td>\n",
              "      <td>1.106548</td>\n",
              "      <td>1447.0</td>\n",
              "      <td>1.605993</td>\n",
              "      <td>37.63</td>\n",
              "      <td>-122.43</td>\n",
              "      <td>1.442</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5.3275</td>\n",
              "      <td>5.0</td>\n",
              "      <td>6.490060</td>\n",
              "      <td>0.991054</td>\n",
              "      <td>3464.0</td>\n",
              "      <td>3.443340</td>\n",
              "      <td>33.69</td>\n",
              "      <td>-117.39</td>\n",
              "      <td>1.687</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3.1000</td>\n",
              "      <td>29.0</td>\n",
              "      <td>7.542373</td>\n",
              "      <td>1.591525</td>\n",
              "      <td>1328.0</td>\n",
              "      <td>2.250847</td>\n",
              "      <td>38.44</td>\n",
              "      <td>-122.98</td>\n",
              "      <td>1.621</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7.1736</td>\n",
              "      <td>12.0</td>\n",
              "      <td>6.289003</td>\n",
              "      <td>0.997442</td>\n",
              "      <td>1054.0</td>\n",
              "      <td>2.695652</td>\n",
              "      <td>33.55</td>\n",
              "      <td>-117.70</td>\n",
              "      <td>2.621</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.0549</td>\n",
              "      <td>13.0</td>\n",
              "      <td>5.312457</td>\n",
              "      <td>1.085092</td>\n",
              "      <td>3297.0</td>\n",
              "      <td>2.244384</td>\n",
              "      <td>33.93</td>\n",
              "      <td>-116.93</td>\n",
              "      <td>0.956</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2.9583</td>\n",
              "      <td>50.0</td>\n",
              "      <td>5.380282</td>\n",
              "      <td>1.117371</td>\n",
              "      <td>579.0</td>\n",
              "      <td>2.718310</td>\n",
              "      <td>33.98</td>\n",
              "      <td>-118.06</td>\n",
              "      <td>1.726</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>3.5200</td>\n",
              "      <td>23.0</td>\n",
              "      <td>4.698217</td>\n",
              "      <td>1.034294</td>\n",
              "      <td>2202.0</td>\n",
              "      <td>3.020576</td>\n",
              "      <td>34.14</td>\n",
              "      <td>-118.01</td>\n",
              "      <td>1.873</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2.7188</td>\n",
              "      <td>32.0</td>\n",
              "      <td>5.511628</td>\n",
              "      <td>1.067829</td>\n",
              "      <td>1337.0</td>\n",
              "      <td>2.591085</td>\n",
              "      <td>34.94</td>\n",
              "      <td>-120.42</td>\n",
              "      <td>1.337</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2.6563</td>\n",
              "      <td>26.0</td>\n",
              "      <td>4.294893</td>\n",
              "      <td>1.123558</td>\n",
              "      <td>1401.0</td>\n",
              "      <td>2.308072</td>\n",
              "      <td>37.68</td>\n",
              "      <td>-122.08</td>\n",
              "      <td>1.841</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1.6944</td>\n",
              "      <td>11.0</td>\n",
              "      <td>21.372093</td>\n",
              "      <td>4.627907</td>\n",
              "      <td>69.0</td>\n",
              "      <td>1.604651</td>\n",
              "      <td>40.19</td>\n",
              "      <td>-121.08</td>\n",
              "      <td>1.375</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   MedInc  HouseAge   AveRooms  ...  Latitude  Longitude  MedianHouseValue\n",
              "0  3.5214      15.0   3.049945  ...     37.63    -122.43             1.442\n",
              "1  5.3275       5.0   6.490060  ...     33.69    -117.39             1.687\n",
              "2  3.1000      29.0   7.542373  ...     38.44    -122.98             1.621\n",
              "3  7.1736      12.0   6.289003  ...     33.55    -117.70             2.621\n",
              "4  2.0549      13.0   5.312457  ...     33.93    -116.93             0.956\n",
              "5  2.9583      50.0   5.380282  ...     33.98    -118.06             1.726\n",
              "6  3.5200      23.0   4.698217  ...     34.14    -118.01             1.873\n",
              "7  2.7188      32.0   5.511628  ...     34.94    -120.42             1.337\n",
              "8  2.6563      26.0   4.294893  ...     37.68    -122.08             1.841\n",
              "9  1.6944      11.0  21.372093  ...     40.19    -121.08             1.375\n",
              "\n",
              "[10 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dCP0xObH8ar",
        "colab_type": "text"
      },
      "source": [
        "Now that we have the seperate files, let's create a dataset with all these files within it. The tf.datat.Dataset.list_files() shuffles the files for you."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRSJcGKKHds1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "fcb6bdc5-51d1-438f-d107-7b4f9d9b7dc3"
      },
      "source": [
        "filepath_dataset = tf.data.Dataset.list_files(train_filepaths, seed= 42)\n",
        "for file in filepath_dataset:\n",
        "  print(file)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(b'datasets/housing/california_train_05.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/california_train_16.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/california_train_01.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/california_train_17.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/california_train_00.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/california_train_14.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/california_train_10.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/california_train_02.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/california_train_12.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/california_train_19.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/california_train_07.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/california_train_09.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/california_train_13.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/california_train_15.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/california_train_11.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/california_train_18.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/california_train_04.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/california_train_06.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/california_train_03.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/california_train_08.csv', shape=(), dtype=string)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qgJYQU4InF5",
        "colab_type": "text"
      },
      "source": [
        "You can now call interleave which reads from n files at a time. Skip the header and then print the first few lines of the new dataset. Notice the shuffle of the rows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZ_4tiJuITa6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "cb3bef3b-032e-44c2-ad27-5025325e9a3e"
      },
      "source": [
        "n_files = 5\n",
        "dataset = filepath_dataset.interleave(lambda filepath:\n",
        "                                      tf.data.TextLineDataset(filepath).skip(1),\n",
        "                                      cycle_length= n_files)\n",
        "for line in dataset.take(5):\n",
        "  print(line.numpy()) #returns a byte string so we need to convert it"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'4.5909,16.0,5.475877192982456,1.0964912280701755,1357.0,2.9758771929824563,33.63,-117.71,2.418'\n",
            "b'2.4792,24.0,3.4547038327526134,1.1341463414634145,2251.0,3.921602787456446,34.18,-118.38,2.0'\n",
            "b'4.2708,45.0,5.121387283236994,0.953757225433526,492.0,2.8439306358381504,37.48,-122.19,2.67'\n",
            "b'2.1856,41.0,3.7189873417721517,1.0658227848101265,803.0,2.0329113924050635,32.76,-117.12,1.205'\n",
            "b'4.1812,52.0,5.701388888888889,0.9965277777777778,692.0,2.4027777777777777,33.73,-118.31,3.215'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJFHKGWlKjCo",
        "colab_type": "text"
      },
      "source": [
        "#### Preprocessing - Parse Byte String and Scaling the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiU-PAwTJp4p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c3572111-40a6-4316-9b36-eddd3cbcc5c3"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11610, 8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04UjRUNXdvSE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "#standard scale inputs, part of preprocessing of the data\n",
        "std_scaler = StandardScaler()\n",
        "std_scaler.fit(X_train)\n",
        "X_mean = std_scaler.mean_\n",
        "X_std = std_scaler.scale_ #np.sqrt(var_)\n",
        "\n",
        "n_inputs = 8 # X_train.shape[:-1]\n",
        "\n",
        "@tf.function\n",
        "def preprocess(line):\n",
        "  defaults = [0.] * n_inputs + [tf.constant([], dtype= tf.float32)] #number of columns and their types\n",
        "  #default value will be 0.\n",
        "  #the last column contains floats but raises an exception if their is a missing value\n",
        "  fields = tf.io.decode_csv(line, record_defaults = defaults)\n",
        "  \n",
        "  #we need to stack the list of scalar tensors\n",
        "  X = tf.stack(fields[:-1])\n",
        "  y = tf.stack(fields[-1:])\n",
        "  return (X- X_mean) / X_std, y\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-TpIDnTe_dx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "b8ee6610-c36b-4b20-b08a-630d1723021b"
      },
      "source": [
        "# using the data from below \n",
        "\n",
        "preprocess(b'2.1856,41.0,3.7189873417721517,1.0658227848101265,803.0,2.0329113924050635,32.76,-117.12,1.205')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(8,), dtype=float32, numpy=\n",
              " array([-0.8936168 ,  0.9789995 , -0.6810549 , -0.07264194, -0.5669866 ,\n",
              "        -0.39212453, -1.3522334 ,  1.2316071 ], dtype=float32)>,\n",
              " <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.205], dtype=float32)>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMI6SowbfCww",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#apply this to enitre dataset across the multiple files\n",
        "\n",
        "def csv_reader_dataset(filepath, repeat=1, n_readers=5, n_read_threads= None,\n",
        "                        shuffle_buffer_size=10000, n_parse_threads=5,\n",
        "                        batch_size=32):\n",
        "  dataset = tf.data.Dataset.list_files(filepath).repeat(repeat)\n",
        "  dataset = dataset.interleave(lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
        "                               cycle_length=n_readers, num_parallel_calls= n_read_threads)\n",
        "  \n",
        "  #map each line \n",
        "  dataset = dataset.map(preprocess, num_parallel_calls= n_parse_threads)\n",
        "  dataset = dataset.shuffle(shuffle_buffer_size).repeat(repeat)\n",
        "  return dataset.batch(batch_size).prefetch(1) #for performance"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIM_AS8vs7LV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "d3b692d7-a9dc-43bf-f1d1-9d732442395f"
      },
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "train_set = csv_reader_dataset(train_filepaths, batch_size=3)\n",
        "for X_batch, y_batch in train_set.take(2):\n",
        "  print(X_batch)\n",
        "  print()\n",
        "  print(y_batch)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[ 0.5804519  -0.20762321  0.05616303 -0.15191229  0.01343246  0.00604472\n",
            "   1.2525111  -1.3671792 ]\n",
            " [ 5.818099    1.8491895   1.1784915   0.28173092 -1.2496178  -0.3571987\n",
            "   0.7231292  -1.0023477 ]\n",
            " [-0.9253566   0.5834586  -0.7807257  -0.28213993 -0.36530012  0.27389365\n",
            "  -0.76194876  0.72684526]], shape=(3, 8), dtype=float32)\n",
            "\n",
            "tf.Tensor(\n",
            "[[1.752]\n",
            " [1.313]\n",
            " [1.535]], shape=(3, 1), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[-0.8324941   0.6625668  -0.20741376 -0.18699841 -0.14536144  0.09635526\n",
            "   0.9807942  -0.67250353]\n",
            " [-0.62183803  0.5834586  -0.19862501 -0.3500319  -1.1437552  -0.3363751\n",
            "   1.107282   -0.8674123 ]\n",
            " [ 0.8683102   0.02970133  0.3427381  -0.29872298  0.7124906   0.28026953\n",
            "  -0.72915536  0.86178064]], shape=(3, 8), dtype=float32)\n",
            "\n",
            "tf.Tensor(\n",
            "[[0.919]\n",
            " [1.028]\n",
            " [2.182]], shape=(3, 1), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WA887Ds9v-PY",
        "colab_type": "text"
      },
      "source": [
        "# Prefetching\n",
        "By calling the prefetch function we allow TensorFlow to work one batch ahead. By calling num_parallel_calls we ensuring that we are loading and preprocessing in a multithreaded manor. Taking full advantage of CPU and GPU available. \n",
        "\n",
        "There are many more details and functions available that can be utilised for datasets, even a few experiemental ones - check tf.data.experimental. \n",
        "\n",
        "# Datasets with tf.keras\n",
        "In most cases, the CSV function will be sufficient and you should not try convert this to TFRecord format. This is preferred format for large datasets. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BcLjWj9xI3y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_set = csv_reader_dataset(train_filepaths, repeat=None)\n",
        "valid_set = csv_reader_dataset(valid_filepaths)\n",
        "test_set = csv_reader_dataset(test_filepaths)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjqo4assxzy0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8a3d4835-8ae6-4108-cb53-d2dac53e94c4"
      },
      "source": [
        "train_set #1x8 feature matrix and 1D vector"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset shapes: ((None, 8), (None, 1)), types: (tf.float32, tf.float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYujyBFiy6Tf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "884849b4-2849-4ad5-bae6-b190e6ac5cf0"
      },
      "source": [
        "keras.backend.clear_session()\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "keras.layers.Dense(30, activation= 'relu', input_shape= X_train.shape[1:]),\n",
        "keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(loss= 'mse', optimizer= 'nadam')\n",
        "\n",
        "model.fit(train_set, steps_per_epoch= len(X_train) // 32, epochs= 10,\n",
        "          validation_data= valid_set)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "362/362 [==============================] - 1s 3ms/step - loss: 1.0880 - val_loss: 7.5127\n",
            "Epoch 2/10\n",
            "362/362 [==============================] - 1s 3ms/step - loss: 0.5499 - val_loss: 1.2298\n",
            "Epoch 3/10\n",
            "362/362 [==============================] - 1s 3ms/step - loss: 0.4429 - val_loss: 0.5427\n",
            "Epoch 4/10\n",
            "362/362 [==============================] - 1s 3ms/step - loss: 0.4080 - val_loss: 0.4023\n",
            "Epoch 5/10\n",
            "362/362 [==============================] - 1s 3ms/step - loss: 0.3944 - val_loss: 0.3807\n",
            "Epoch 6/10\n",
            "362/362 [==============================] - 1s 3ms/step - loss: 0.4000 - val_loss: 0.4626\n",
            "Epoch 7/10\n",
            "362/362 [==============================] - 1s 3ms/step - loss: 0.3715 - val_loss: 0.5092\n",
            "Epoch 8/10\n",
            "362/362 [==============================] - 1s 3ms/step - loss: 0.3743 - val_loss: 0.3962\n",
            "Epoch 9/10\n",
            "362/362 [==============================] - 1s 3ms/step - loss: 0.3752 - val_loss: 0.3481\n",
            "Epoch 10/10\n",
            "362/362 [==============================] - 1s 3ms/step - loss: 0.3771 - val_loss: 0.3547\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fdac66a64a8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_ldc0gb4S58",
        "colab_type": "text"
      },
      "source": [
        "# TFRecord Format\n",
        "\n",
        "TensorFlow's preferred format for large datasets. It consists of a list of binary records. \n",
        "\n",
        "The functions available allow you to read files in parallel and compress files so they can be loaded via a network connection (create a compressed TFRecord file - options = tf.io.TFRecordOptions(compression_type= 'GZIP). To read this you need to specify the compression type (compression_type= 'GZIP')."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjYPYOJWz4yp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create a TFRecord file\n",
        "with tf.io.TFRecordWriter('tfrecord') as f:\n",
        "  f.write(b'First line')\n",
        "  f.write(b'Second line') #input data that is in binary\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JruYr-wR5wal",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ca7b40a8-fb1f-4c71-e7dc-4ca4298808bd"
      },
      "source": [
        "#read from the TFRecord file\n",
        "\n",
        "file = ['tfrecord']\n",
        "dataset = tf.data.TFRecordDataset(file)\n",
        "for line in dataset:\n",
        "  print(line)\n",
        "\n",
        "#you can read multiple simultaneously, reading it parallel and interleave their records"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(b'First line', shape=(), dtype=string)\n",
            "tf.Tensor(b'Second line', shape=(), dtype=string)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZjwpJfP_RtG",
        "colab_type": "text"
      },
      "source": [
        "#### Protocol Buffers\n",
        "\n",
        "Every TFRecord file contains **serialized protocol buffers (protobufs)- a portable, extension and efficient binary format** developed by Google in 2001.\n",
        "\n",
        "TensorFlow provides a protobuf functions for parsing operations. You can load images and decode many formats - parse data using tf.io.parse_tensor() and store any tensor using tf.io.serialize_tensor(). \n",
        "\n",
        "## Preprocessing using a Data API and preprocessing layer\n",
        "Instead of processing your data in Pandas, Sci-kit learn, NumPy, you can use Data API to process the data on the fly as it loads it (i.e. using tf.datasets map function) or by creating a custom preprocessing layer. You can use keras.layers.Lambda or create a custom class - by defining the .adapt() method and passing a data sample to be then used as a normal layer.\n",
        "\n",
        "##### Categorical Encoding\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxUbWmuH5yZJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "age_mean, age_std = X_mean[1], X_std[1]\n",
        "housing_median_age = tf.feature_column.numeric_column('housing_median_age',\n",
        "normalizer_fn=lambda x: (x-age_mean)/ age_std)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1piqgNJ8QW5l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4a5053e7-581d-4d62-f17a-1e2823543dad"
      },
      "source": [
        "housing_median_age"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NumericColumn(key='housing_median_age', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=<function <lambda> at 0x7fdacf4e7e18>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    }
  ]
}